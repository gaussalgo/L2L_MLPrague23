{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Transformers\n",
    "1. **A description of a Transformer model**\n",
    "2. **Pre-training a language model**\n",
    "3. **Fine-tuning a model for tasks**\n",
    "4. **How to manipulate the output, without re-training the model (generation strategies etc.)**\n",
    "5. **Hands-on: Logit Processors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers[sentencepiece]==4.19.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (4.19.1)\n",
      "Requirement already satisfied: filelock in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (2023.5.5)\n",
      "Requirement already satisfied: requests in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (2.30.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (4.65.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (4.23.1)\n",
      "Requirement already satisfied: fsspec in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]==4.19.1) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]==4.19.1) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.19.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.19.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.19.1) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.19.1) (2023.5.7)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n",
      "Requirement already satisfied: wheel in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers[sentencepiece]==4.19.1\n",
    "!{sys.executable} -m pip install torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Transformers?\n",
    "\n",
    "The transformers structure was introduced regarding the task of Machine Translation. Models, that have been used for this task prior were RNN's (recurrent neural networks) models. Their main problems were: being slow and dropping performace with longer sentences (inputs).\n",
    "\n",
    "What do the Transformers do differently?\n",
    "* The input sentence is processed at once, where the RNN and LSTM processed the sentence word by word. This allows for parallelization and thus improving the speed \n",
    "* Using the \"self-attention\" and the sentence being processed at once helps to keep the information about dependencies intact, where the RNN and LSTM approach was that the dependencies with the other words were passed down in the hidden states and as such the information was lost the longer the output sentence was.\n",
    "* Apart from having word embeddings, which is it vector representation the transformers also have positional embeddings, because the sentence is no longer processed word by word, so we need a different way to include the information about the position of the word in the sentence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO get rid of LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üèûÔ∏è The journey from the input..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to tokenize the input. Tokenizers are a dictionary where each word, or a syllable has an ID. You always need to use the tokenizer that was used during the pre-training and fine-tuning of your model, otherwise you will get complete gibberish. The tokenizer also has special tokens such as EOS (end of sentence), BOS (beggining of sentence) etc.\n",
    "\n",
    "When training (or even in inference) it is possible, that you will want to process multiple inputs at once. As the model works with tensors, we need the inputs tokenized to input_ids of the same length. Becuase of that, the tokenized input also has the \"attention mask\" which is an indicator which parts of the tokenized sequence is relevant and which is only the padding for ensuring the same length requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output: ['This is the supercalifragilisticexpialidocious input text.', 'second text']\n",
      "Tokenized output (IDs): [[100, 19, 8, 1355, 15534, 20791, 173, 3040, 994, 102, 23, 4288, 7171, 2936, 3785, 1499, 5, 1], [511, 1499, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Tokenized output (attention mask): [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Tokenized output (in tokens) - 1st sequence: ['‚ñÅThis', '‚ñÅis', '‚ñÅthe', '‚ñÅsuper', 'cali', 'frag', 'il', 'istic', 'ex', 'p', 'i', 'ali', 'doc', 'ious', '‚ñÅinput', '‚ñÅtext', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-small\") # Download the corresponding tokenizer\n",
    "input_sentence = [\"This is the supercalifragilisticexpialidocious input text.\", \"second text\"]\n",
    "\n",
    "# We enable padding, as we need both sentence tokenized to the sequence of the same length\n",
    "tokenized_input = tokenizer_t5(input_sentence, padding=True)\n",
    "\n",
    "print(\"Original output: {}\".format(input_sentence))\n",
    "print(\"Tokenized output (IDs): {}\".format(tokenized_input.input_ids))\n",
    "\n",
    "# attention mask vector provides information on which parts of the sample are relevant (are not padding)\n",
    "print(\"Tokenized output (attention mask): {}\".format(tokenized_input.attention_mask))\n",
    "\n",
    "# The tokenizer has a way of dealing with words which are not in its vocab.\n",
    "# Here if the word start after a whitespace it has \"_\" in front whereas if the word has to be split its parts have tokens without the \"_\"\n",
    "print(\"Tokenized output (in tokens) - 1st sequence: {}\".format(tokenizer_t5.convert_ids_to_tokens(tokenized_input.input_ids[0])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Embeddings\n",
    "The tokenized input then needs to be transformed into a vector representation. This representantion needs to hold information about the semantic property of a word (*Input Embedding*) and positional information about where the word is in the input sequence (*Positional Embedding*). Each word's embedding has a size of a hidden layer (default 512). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the input ids tensor: torch.Size([1, 18])\n",
      "Size of the word embedding for the input sequence: torch.Size([1, 18, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "input_sentence = [\"This is the supercalifragilisticexpialidocious input text.\"]\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "input_ids = tokenizer_t5(input_sentence, return_tensors=\"pt\")\n",
    "outputs=model_t5.generate(input_ids.input_ids, return_dict_in_generate=True,output_hidden_states=True, output_attentions=True, output_scores=True)\n",
    "\n",
    "#encoder hidden states contain word embedding and then all outputs of each encoder layer\n",
    "word_embeddings_for_input_sentence = outputs.encoder_hidden_states[0] \n",
    "print(\"Size of the input ids tensor: {}\".format(input_ids.input_ids.shape))\n",
    "print(\"Size of the word embedding for the input sequence: {}\".format(word_embeddings_for_input_sentence.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 What is Self-attention?\n",
    "Self-attention layer is a building block of the Transformer infrastructure, which is able to include the information about the connection between  individual words/tokens from the input sequence. Below you can see how the attention score is computed\n",
    "\n",
    "![attention_score.png](https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n",
    "[[source]](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "Input and output of every self-attention layer with be embeddings / hidden states.\n",
    "\n",
    "The embeddings are passed to three separate linear layers, which compute the desired matrices **query matrix Q**, **key matrix K** and a **value matrix V** by multiplying the embbedding by their own weights ($W^{Q}$, $W^{K}$, $W^{V}$).  \n",
    "\n",
    "Each word queries the scores to all words from the input sequence, this is done by matrix multiplication of $Q$ and $K^T$. The results are divided (for stability purposes) and a softmax function (values in a row ad up to 1) is applied. The result is then multiplied with the matrix $V$. This applies the attention scores to the corresponding values (imagine $q_1$ being applied to all of matrix $K$ to be multiplied by $v_1$)\n",
    "\n",
    "#### 1.2.1 Multi-head attention\n",
    "Instead of computing only one attention score we use the multi-head attention. What changes?\n",
    "* ($W^{Q}$, $W^{K}$, $W^{V}$) are split and resized into a tuple of smaller ($W^{Q}$, $W^{K}$, $W^{V}$) for each head\n",
    "* in each head Q, K and V is computed and the attention score is computed using $d_k$ = embedding size/ # heads\n",
    "* the results for each head are concatonated together to again by of the size as the input embedding.\n",
    "\n",
    "Allowing multiple attention score to be computed we allow for a richer representation.\n",
    "\n",
    "#### 1.2.2 Types of self-attention layers\n",
    "\n",
    "* Encoder self-attention (Multi-head attention) - Q,K and V are computed from the input sequence embedding or a hidden state of a previous encoder\n",
    "* Decoder self-attention (Masked multi-head attention)- Q,K and V are computed from the target/output sequence embedding or a hidden state of a previous decoder\n",
    "* Encoder-Decoder self-attention (Multi-head attention)- Q is computed from the target/output sequence embedding or a hidden state of a previous decoder, K and V are from the last hidden state of the Encoder\n",
    "\n",
    "![attentions.png](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n",
    "[[source]](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.üí™ Pre-training & Fine-tuning\n",
    "The magic of Deep language models lays in their good preliminary knowledge of language. They obtain this knowledge during so-called pre-training phase, where they are trained for a different token classification task an instance of Language Modeling.\n",
    "\n",
    "\n",
    "### 2.1 Language Modeling\n",
    "\n",
    "In the instances of Language Modeling task, models are asked to solve a task of 'guessing' the **right word in the context**.\n",
    "\n",
    "This task comes in two main instances:\n",
    "\n",
    "* **Masked Language Modeling (MLM)**: Models guess the correct token **within context**. This objective best prepares the model for **classification tasks** (Named Entity Recognition, Sequence Classification)\n",
    "\n",
    "![image.png](https://www.rohanawhad.com/content/images/size/w1600/2022/04/image.png)\n",
    "[[source]](https://www.rohanawhad.com/improvements-of-spanbert-over-bert/)\n",
    "\n",
    "* **Causal Langauge Modeling (CLM)**: Models guess the **following token** from previous context. This objective is better for preparing the model for **generation**, such as Dialogue, Summarization, Translation.\n",
    "\n",
    "![image.png](https://gcdnb.pbrd.co/images/Bx4h6Lordx0y.png?o=1)  \n",
    "![image.png](https://gcdnb.pbrd.co/images/rb7bmZS11gtl.png?o=1)\n",
    "![image.png](https://gcdnb.pbrd.co/images/gXYffjzLIk7n.png?o=1)\n",
    "\n",
    "Other variances: Synthetic token classification (ALBERT), Mask infilling (BART, T5), Sequence reconstruction (BART), Sequence Classification (BERT).\n",
    "\n",
    "### 2.2 Fine-tuning\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO nice graph for CLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.üí°Specific tasks\n",
    "As stated above Transformers can be used in multiple ways, depending on what the desired output is. We can either use the whole architecture as shown above or use only the decoder or the encoder. In every task the input is sequence of tokens. The output and how we aquire it depends on which task we choose. However we are able to sort them into two categories: **Token classification** and **Text generation**\n",
    "\n",
    "### 3.1 Token classification\n",
    "For these tasks we need to have the best representation for each word within its context(left and right). Model pretrained on tasks such as MLM are best suited for token classification. For the classification itself, we want the best representation of the input sequence and therefore we use the last hidden state of the encoder. \n",
    "\n",
    "* **Text classification (positive/negative)**: Before the sequence we wish to classify we add a $[CLS]$ token. After recieving the hidden states,we process them through a linear layer and based on the value for the $[CLS]$ token we classify the sentence.\n",
    "\n",
    "* **Token classification (NER)**: Linear layer processes the hidden states to logits, where cross-entropy loss is computed, to return a most probable label for each token.\n",
    "\n",
    "* **Extractive QA**: Linear layer processes the hidden states to logits, where cross-entropy loss is computed, to find the best candidates for start and end of the answer tokens in the sequence.\n",
    "\n",
    "### 3.2 Text generation\n",
    "With these tasks the end goal is to guess the most probable token following the previous tokens. Model pre-trained on tasks similar to CLM are best suitable for text generation task. The following tasks have a certain overlap, as they both have text as input and output. These tasks need next token generation, which is provided by the decoder and as such can be performed by either a traditional Encoder-Decoder or Decoder-only Transformer.\n",
    "\n",
    "In both cases the embeddings are processed (either through a ED or D transformer) and the decoder hidden states are processed by a linear layer and we get logits from which we determine the most probable token.\n",
    "\n",
    "\n",
    "* Text generation (Completion, code generation): Given a sequence of words, the model generates the next word. This task can be trained on unlabeled data and is often used with Decoder-only transformers. \n",
    "* Sequence to sequence (Summarization, Translation, etc.): These tasks need the model to learn to map pairs of text (english to german, article to abstract). These tasks are mainly about transcribing the input into a different text (translate to a different language / simplify the text but maintain the meaning) and benefit from understanding the input sequence. Because of thatthey are used mainly with Encoder-Decoder models.\n",
    "\n",
    "![animation.gif](https://jalammar.github.io/images/t/transformer_decoding_2.gif)\n",
    "[[source]](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "### 3.3 Decoder or Encoder-Decoder?\n",
    "With the rise of ChatGPT and seing its capabilities it is a valid question if a GPT (Decoder only) model would not suffice even on tasks, previously though better suited for a seq2seq task. A [Paper comparing Encoder-Decoder and Decoder-only models](https://arxiv.org/pdf/2304.04052.pdf) on machine translation showed some reasons, why we shouldn't focus only on decoder-only large language models for all text generation tasks. \n",
    "\n",
    "Reasons for using Decoder-only models:\n",
    "* Smaller size and can be trained on much more data (unsupervised)\n",
    "\n",
    "Reasons against using Decoder-only models:\n",
    "* No encoder hidden states in the decoder self-attention layers\n",
    "* The only information about the input sequence is in the decoder hidden states\n",
    "This results in more hallucination with the growing index of generation and the attention degeneration, as the hidden states cannot hold information about the input sequence\n",
    "\n",
    "The best rule-of-thunb would be: If you need the model to not diverge from initial input -> Encoder-Decoder. Would you greatly benefit from exposing the model to a large quantity of data and diverging from the input sequence meaning is not a massive drawback for you -> Decoder-only"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üèûÔ∏è The journey from the input...to the output - Text Generation\n",
    "\n",
    "Even though it may seem, that once we have a trained model we are done, there is the generation itself. While the weights in the decoder (and encoder) are set, we can still tweak the generated output, by using multiple generation strategies as well as using Logit Processors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 üó∫Ô∏è Decoding strategies\n",
    "While iteratively generating the output tokens, there are multiple strategies, that can be used to achieve the \"best\" output text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "input_sentence = \"Two thousand years ago, the \"\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "input = tokenizer_gpt(input_sentence, return_tensors=\"pt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Greedy search**: At each iteration, the most probable token is generated (most probable token at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the vernacular of the ancient Egyptians was the language of the gods. The language of the gods was the language of the gods. The language of the gods was the language of the gods. The language of the gods was']\n"
     ]
    }
   ],
   "source": [
    "outputs=model_gpt.generate(**input, early_stopping=True, max_length=50) #greedy search\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Beam search**: At each iteration $N$ number of beams with the best overall probability are stored. After the generation is complete the beam with highes overall probability is returned (most probable output as a whole)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the vernal equinox was the first time that the sun and moon met, and the sun and moon were the first time that the sun and moon met. The sun and moon were the first time that the sun', 'Two thousand years ago, the vernal equinox was the first time that the sun and moon met, and the sun and moon were the first time that the sun and moon met.\\n\\nThe sun and moon were the first time that']\n"
     ]
    }
   ],
   "source": [
    "outputs=model_gpt.generate(**input, num_beams=2, num_return_sequences=2, max_length=50) #beam search\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Multinomial sampling**: sampling the token distribution \n",
    "* **Top-$K$ sampling**: sampling only $K$ samples with the highest probabilities.  \n",
    "* **Nucleus sampling (top-$p$ sampling)**: Computing a cumulative distribution function and sampling only till the cut-off at the $p$ quantile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the vernacular for the medieval city of Kynsgaard in Estonia lay in the form of a large stone gate to the city. In the past 200 years, it has undergone its own expansion. It is in this']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the ichthyologists and palaeobiologists made a breakthrough, by using a method that mimicked the behavior of some animal dinosaurs and allowed them to see how long fossils were formed in a natural system. Their result is']\n",
      "['Two thousand years ago, the vernacular of the people of the land was the language of the people. The people of the land were the people of the land. The people of the land were the people of the land. The people of the']\n"
     ]
    }
   ],
   "source": [
    "outputs=model_gpt.generate(**input, do_sample=True, max_length=50, early_stopping=True) # multinomial sampling\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))\n",
    "outputs=model_gpt.generate(**input, do_sample=True, top_k=50, max_length=50, early_stopping=True) # Top-k sampling\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))\n",
    "outputs=model_gpt.generate(**input, do_sample=True, top_p=0.1, top_k=0, max_length=50, early_stopping=True) #Top-p sampling\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Temperature**: A parameter which adjusts the logits before applying the softmax function. Greater temperature can lead to less probable words generated (more unexpected generated sequence), small temperature will lead to more conservative outputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the urn was found, and the Gods of Earth and Earthlings worshiped it. The gods of Earth, and of the gods of Earth, are the Gods of the Void.\\n\\nThe Void is an alternate reality']\n",
      "['Two thousand years ago, the vernacular of the ancient Egyptians was the language of the gods. The language of the gods was the language of the gods. The language of the gods was the language of the gods. The language of the gods was']\n"
     ]
    }
   ],
   "source": [
    "outputs=model_gpt.generate(**input, early_stopping=True, max_length=50, do_sample=True, top_k=0, temperature=0.7) #multinomial sampling with high temp\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))\n",
    "outputs=model_gpt.generate(**input, early_stopping=True, max_length=50, do_sample=True, top_k=0, temperature = 0.001) #multinomial sampling with low temp\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 üëÆ Logit Processors\n",
    "Logit processors can be applied to the computed logits, to alter them and therefore change the next most probable token. They are not dependant on any decoding strategy. Some of the basic usages are enforcing minimal or maximal length of the output, forbidding specific words etc. Here we will show you a naive approach to creating a Logit processor which enforces a token that should be in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Two thousand years ago, the vernal equinox was a time of great stupidity stupidness. It stupidened stupid people and made stupid people dumb.\\n\\nBut now it stupidens everyone stupidly. And that stupidness has stupidened everybody stupidly because stupid people don't want stupid things to happen. They stupidly believe stupid things will be done stupidly if they do stupid things in stupid ways. So stupid people have stupid beliefs stupidly about stupid things. That's why I stupidly stupid myself stupidly.\\n\\nThe stupidest thing you can do is not stupid at stupid times. You just stupid yourself stupidly when you think stupid things are going on stupidly\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import LogitsProcessor, LogitsProcessorList, BatchEncoding\n",
    "import torch\n",
    "\n",
    "class EnforceWordProcessor(LogitsProcessor):\n",
    "    def __init__(self, desired_input: BatchEncoding, param: float = 200.):\n",
    "        assert len(desired_input.input_ids) ==1\n",
    "        self.desired_input = desired_input.input_ids[0]\n",
    "        self.param = param #the parameter which indicates how much we want to enforce the word\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        last_id = input_ids[-1][-1] # ID of the last generated token (whose score we are adjusting)\n",
    "        if last_id != self.desired_input:\n",
    "            scores[-1,self.desired_input] = scores[-1,self.desired_input]/self.param\n",
    "        else:\n",
    "            scores[-1,self.desired_input] = -float(\"inf\") # we only want to generate the token once\n",
    "        return scores\n",
    "    \n",
    "enforced_word = \" stupid\" # we need the whitespace for the tokenizer to understand this as one token only\n",
    "\n",
    "custom_processor = EnforceWordProcessor(tokenizer_gpt(enforced_word, add_special_tokens=False))\n",
    "\n",
    "inputs = tokenizer_gpt(input_sentence, return_tensors=\"pt\")\n",
    "\n",
    "#repetition penalty is needed as the naive Logit Processor leads to repetition, so to avoid that, we include a rep. penalty\n",
    "# Here we are using a beam search decodeing strategy, which is much more likely to fall into the pit of repetition\n",
    "outputs = model_gpt.generate(**inputs, logits_processor=LogitsProcessorList([custom_processor]), repetition_penalty = 5., max_length=140, num_beams=5)\n",
    "output_text = tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "output_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are Logit Processors worth the work?\n",
    "\n",
    "The are use-cases, where a logit processor can help you enforce a rule of how structured the output should be. A really good example of that is the [**jsonformers**](https://github.com/1rgs/jsonformer) repo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ‚úã Hands-on: Adjusting the output without retraining using Logit Processors\n",
    "\n",
    "In this first short session we would ask you, to take the previously shown naive Logit Processor and try to adjust it to perform better by: \n",
    "* enforce not a single token word, but a sequence of multiple tokens.\n",
    "* Try to fix the repetition problem without using a repetition penalty\n",
    "\n",
    "You can also experiment with different decoding strategies, which we talked about above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This is the start of the sequence.\\n\\nThe first thing to do is to create a new']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LogitsProcessor, LogitsProcessorList\n",
    "import torch\n",
    "\n",
    "class YourVeryOwnLogitProcessor(LogitsProcessor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        # The __call__ method has to return scores\n",
    "        return scores\n",
    "   \n",
    "custom_processor = YourVeryOwnLogitProcessor()\n",
    "\n",
    "text = \"This is the start of the sequence\"\n",
    "\n",
    "inputs = tokenizer_gpt(text, return_tensors=\"pt\") # tokenize the input sequence\n",
    "outputs = model_gpt.generate(**inputs, logits_processor=LogitsProcessorList([custom_processor])) # generate the output - now using greedy search\n",
    "output_text = tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True) # decode the output\n",
    "\n",
    "output_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
