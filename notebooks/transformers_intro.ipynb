{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Transformers\n",
    "\n",
    "1. **A description of a Transformer model**\n",
    "2. **Pre-training a language model**\n",
    "3. **Overview of specific tasks**\n",
    "4. **How to manipulate the output, without re-training the model (generation strategies etc.)**\n",
    "5. **Hands-on: Logit Processors**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece]==4.19.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (4.19.1)\n",
      "Requirement already satisfied: filelock in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (2023.5.5)\n",
      "Requirement already satisfied: requests in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (2.30.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (4.65.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.19.1) (4.23.1)\n",
      "Requirement already satisfied: fsspec in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]==4.19.1) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]==4.19.1) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.19.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.19.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.19.1) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.19.1) (2023.5.7)\n",
      "Requirement already satisfied: torch in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n",
      "Requirement already satisfied: wheel in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers[sentencepiece]==4.19.1\n",
    "!{sys.executable} -m pip install torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Transformers?\n",
    "\n",
    "The transformers structure was introduced regarding the task of **Machine Translation**. Models, that have been used for this task prior were RNN's (recurrent neural networks) models. Their main problems were: being slow and dropping performance with longer sentences (inputs).\n",
    "\n",
    "What do the Transformers do differently?\n",
    "\n",
    "- The **input sentence is processed at once**, where the RNN processed the sentence word by word. This allows for parallelization and thus improving the speed\n",
    "- Using the **\"self-attention\"** and the sentence being processed at once helps to keep the information about dependencies intact, where the RNN approach was that the dependencies with the other words were passed down in the hidden states and as such the information was lost the longer the output sentence was.\n",
    "- Apart from having word embeddings, which is a vector representation of the input, the transformers also have positional embeddings, because the sentence is no longer processed word by word, so we need a different way to include the information about the position of the word in the sentence.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üèûÔ∏è The journey from the input...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to tokenize the input. Tokenizers are basically a dictionary where each word/ syllable/character has an ID. You **always** need to **use the tokenizer that was used during the pre-training and fine-tuning of your model**, otherwise you will get complete gibberish. The tokenizer also has special tokens such as EOS (end of sentence), BOS (beggining of sentence) etc. (+ you can add special tokens)\n",
    "\n",
    "When training (or in inference) it is possible, that you will want to process multiple inputs at once. As the model works with tensors, we need the inputs tokenized to input_ids of the same length. Because of that, the tokenized input also has the \"attention mask\" which is an indicator which parts of the tokenized sequence are relevant and which are only the padding for ensuring the same length requirement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output: ['This is the supercalifragilisticexpialidocious input text.', 'second text']\n",
      "Tokenized output (IDs): [[100, 19, 8, 1355, 15534, 20791, 173, 3040, 994, 102, 23, 4288, 7171, 2936, 3785, 1499, 5, 1], [511, 1499, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Tokenized output (attention mask): [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Tokenized output (in tokens) - 1st sequence: ['‚ñÅThis', '‚ñÅis', '‚ñÅthe', '‚ñÅsuper', 'cali', 'frag', 'il', 'istic', 'ex', 'p', 'i', 'ali', 'doc', 'ious', '‚ñÅinput', '‚ñÅtext', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-small\") # Download the corresponding tokenizer\n",
    "input_sentence = [\"This is the supercalifragilisticexpialidocious input text.\", \"second text\"]\n",
    "\n",
    "# We enable padding, as we need both sentence tokenized to the sequence of the same length\n",
    "tokenized_input = tokenizer_t5(input_sentence, padding=True)\n",
    "\n",
    "print(\"Original output: {}\".format(input_sentence))\n",
    "print(\"Tokenized output (IDs): {}\".format(tokenized_input.input_ids))\n",
    "\n",
    "# attention mask vector provides information on which parts of the sample are relevant (are not padding)\n",
    "print(\"Tokenized output (attention mask): {}\".format(tokenized_input.attention_mask))\n",
    "\n",
    "# The tokenizer has a way of dealing with words which are not in its vocab.\n",
    "# Here if the word start after a whitespace it has \"_\" in front whereas if the word has to be split its parts have tokens without the \"_\"\n",
    "print(\"Tokenized output (in tokens) - 1st sequence: {}\".format(tokenizer_t5.convert_ids_to_tokens(tokenized_input.input_ids[0])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Embeddings\n",
    "\n",
    "The tokenized input then needs to be transformed into a vector representation. This representantion needs to hold information about the semantic property of a word (_Input Embedding_) and positional information about where the word is in the input sequence (_Positional Embedding_). Each word's embedding has size equivalent of the hidden states (default 512).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the input ids tensor: torch.Size([1, 18])\n",
      "Size of the word embedding for the input sequence: torch.Size([1, 18, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "input_sentence = [\"This is the supercalifragilisticexpialidocious input text.\"]\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "input_ids = tokenizer_t5(input_sentence, return_tensors=\"pt\")\n",
    "outputs=model_t5.generate(input_ids.input_ids, return_dict_in_generate=True,output_hidden_states=True, output_attentions=True, output_scores=True)\n",
    "\n",
    "#encoder hidden states contain word embedding and then all outputs of each encoder layer\n",
    "word_embeddings_for_input_sentence = outputs.encoder_hidden_states[0] \n",
    "print(\"Size of the input ids tensor: {}\".format(input_ids.input_ids.shape))\n",
    "print(\"Size of the word embedding for the input sequence: {}\".format(word_embeddings_for_input_sentence.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 What is Self-attention?\n",
    "\n",
    "Self-attention layer is a building block of the Transformer model, which is able to include the information about the connection between individual words/tokens from the input sequence. Below you can see how the attention score is computed\n",
    "\n",
    "![attention_score.png](https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n",
    "[[source]](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "Input and output of every self-attention layer are embeddings / hidden states.\n",
    "\n",
    "The embeddings are passed to three separate linear layers, which compute the desired matrices **query matrix Q**, **key matrix K** and a **value matrix V** by multiplying the embbedding by their own weights ($W^{Q}$, $W^{K}$, $W^{V}$).\n",
    "\n",
    "Each word queries the scores to all words from the input sequence, this is done by matrix multiplication of $Q$ and $K^T$. The results are divided (for stability purposes) and a softmax function (values in a row ad up to 1) is applied. The result is then multiplied with the matrix $V$. This applies the attention scores to the corresponding values (imagine $q_1$ being applied to all of matrix $K$ to be multiplied by $v_1$)\n",
    "\n",
    "#### 1.2.1 Multi-head attention\n",
    "\n",
    "Instead of computing only one attention score we use the multi-head attention. What changes?\n",
    "\n",
    "- ($W^{Q}$, $W^{K}$, $W^{V}$) are split and resized into a tuple of smaller ($W^{Q}$, $W^{K}$, $W^{V}$) for each head\n",
    "- in each head Q, K and V is computed and the attention score is computed using $d_k$ = embedding size/ # heads\n",
    "- the results for each head are concatonated together and transformed, so they could be used as input for another encoder/decoder.\n",
    "\n",
    "Allowing multiple attention score to be computed we allow for a richer representation.\n",
    "\n",
    "#### 1.2.2 Types of self-attention layers\n",
    "\n",
    "- Encoder self-attention (Multi-head attention) - Q,K and V are computed from the input sequence embedding or a hidden state of a previous encoder\n",
    "- Decoder self-attention (Masked multi-head attention)- Q,K and V are computed from the target/output sequence embedding or a hidden state of a previous decoder\n",
    "- Encoder-Decoder cross-attention (Multi-head attention)- Q is computed from the target/output sequence embedding or a hidden state of a previous decoder, K and V are from the last hidden state of the Encoder\n",
    "\n",
    "![attentions.png](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n",
    "[[source]](https://jalammar.github.io/illustrated-transformer/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.üí™ Pre-training & Fine-tuning\n",
    "\n",
    "The magic of Deep language models lays in their good preliminary knowledge of language. They obtain this knowledge during so-called pre-training phase, where they are trained for a different token classification task an instance of Language Modeling.\n",
    "\n",
    "### 2.1 Language Modeling\n",
    "\n",
    "In the instances of Language Modeling task, models are asked to solve a task of 'guessing' the **right word in the context**.\n",
    "\n",
    "This task comes in two main instances:\n",
    "\n",
    "- **Masked Language Modeling (MLM)**: Models guess the correct token **within context**. This objective best prepares the model for **classification tasks** (Named Entity Recognition, Sequence Classification)\n",
    "\n",
    "![image.png](https://www.rohanawhad.com/content/images/size/w1600/2022/04/image.png)\n",
    "[[source]](https://www.rohanawhad.com/improvements-of-spanbert-over-bert/)\n",
    "\n",
    "- **Causal Langauge Modeling (CLM)**: Models guess the **following token** from previous context. This objective is better for preparing the model for **generation**, such as Dialogue, Summarization, Translation.\n",
    "\n",
    "![CLM_1](images/CLM_1_new.png)  \n",
    "![CLM_2](images/CLM_2_new.png)\n",
    "![CLM_3](images/CLM_3_new.png)\n",
    "\n",
    "Other variances: Synthetic token classification (ALBERT), Mask infilling (BART, T5), Sequence reconstruction (BART), Sequence Classification (BERT).\n",
    "\n",
    "### 2.2 Fine-tuning\n",
    "\n",
    "This is the process of training a pre-trained model for a specific task with new data (usually supervised learning). A specific task corresponds to a \"head\", aka a linear layer, which is trained to weight the last hidden states accordingly.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.üí°Specific tasks\n",
    "\n",
    "As stated above Transformers can be used in multiple ways, depending on what the desired output is. We can either use the whole architecture as shown above or use only the decoder or the encoder. In every task the input is sequence of tokens. The output and how we aquire it depends on which task we choose. However we can sort them into two categories: **Token classification** and **Text generation**\n",
    "\n",
    "### 3.1 Token classification\n",
    "\n",
    "For these tasks we need to have the best representation for each word within its context(left and right). Model pretrained on tasks such as MLM are best suited for token classification. For the classification itself, we want the best representation not a generated token, so we usually use the last hidden state of the encoder.\n",
    "\n",
    "- **Sequence classification (positive/negative)**: Before the sequence we wish to classify we add a $[CLS]$ token. We classify the sentence based on the value for the $[CLS]$ token.\n",
    "\n",
    "- **Token classification (NER)**: For each token we compute cross-entropy loss across all labels classify each token as the most probable label.\n",
    "\n",
    "- **Extractive QA**: For each token we compute cross-entropy loss to find the best candidates for start and end of the answer tokens in the sequence.\n",
    "\n",
    "### 3.2 Text generation\n",
    "\n",
    "With these tasks the end goal is to guess the most probable token following the previous tokens. Model pre-trained on tasks similar to CLM are best suitable for text generation task. The following tasks have a certain overlap, as they both have text as input and output. These tasks need next token generation, which is provided by the decoder and as such can be performed by either a traditional Encoder-Decoder or Decoder-only Transformer.\n",
    "\n",
    "In both cases the embeddings are processed (either through a ED or D transformer) the \"best\" - in some cases most probable token (depends on the generation strategy) is generated.\n",
    "\n",
    "- **Text generation (Completion, code generation)**: Given a sequence of words, the model generates the next word. This task can be trained on unlabeled data and is often used with Decoder-only transformers.\n",
    "- **Sequence to sequence (Summarization, Translation, etc.)**: These tasks need the model to learn to map pairs of text (english to german, article to abstract). These tasks are mainly about transcribing the input into a different text (translate to a different language / simplify the text but maintain the meaning) and benefit from understanding the input sequence. Because of that they are used mainly with Encoder-Decoder models.\n",
    "\n",
    "![animation.gif](https://jalammar.github.io/images/t/transformer_decoding_2.gif)\n",
    "[[source]](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "### 3.3 Decoder or Encoder-Decoder?\n",
    "\n",
    "With the rise of ChatGPT and seing its capabilities it is a valid question if a GPT (Decoder only) model would not suffice even on tasks, previously though better suited for a seq2seq task. A [Paper comparing Encoder-Decoder and Decoder-only models](https://arxiv.org/pdf/2304.04052.pdf) on machine translation showed some reasons, why we shouldn't focus only on decoder-only large language models for all text generation tasks.\n",
    "\n",
    "Reasons for using Decoder-only models:\n",
    "\n",
    "- Smaller size and can be trained on much more data (unsupervised)\n",
    "\n",
    "Reasons against using Decoder-only models:\n",
    "\n",
    "- No encoder hidden states in the decoder self-attention layers\n",
    "- The only information about the input sequence is in the decoder hidden states\n",
    "  This results in more hallucination with the growing index of generation and the attention degeneration, as the hidden states cannot hold information about the input sequence\n",
    "\n",
    "The best rule-of-thunb would be: If you need the model to not diverge from initial input -> Encoder-Decoder. Would you greatly benefit from exposing the model to a large quantity of data and diverging from the input sequence meaning is not a massive drawback for you -> Decoder-only\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üèûÔ∏è The journey from the input...to the output - Text Generation\n",
    "\n",
    "Even though it may seem, that once we have a trained model we are done, there is the generation itself. While the weights in the decoder (and encoder) are set, we can still tweak the generated output, by using multiple generation strategies as well as using Logit Processors.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 üó∫Ô∏è Decoding strategies\n",
    "\n",
    "While iteratively generating the output tokens, there are multiple strategies, that can be used to achieve the \"best\" output text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:921: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "input_sentence = \"Two thousand years ago, the \"\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "input = tokenizer_gpt(input_sentence, return_tensors=\"pt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Greedy search**: At each iteration, the most probable token is generated (most probable token at a time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the vernacular of the ancient Egyptians was the language of the gods. The language of the gods was the language of the gods. The language of the gods was the language of the gods. The language of the gods was']\n"
     ]
    }
   ],
   "source": [
    "outputs=model_gpt.generate(**input, early_stopping=True, max_length=50) #greedy search\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Beam search**: At each iteration $N$ number of beams with the best overall probability are stored. After the generation is complete the beam with highes overall probability is returned (most probable output as a whole)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the vernal equinox was the first time that the sun and moon met, and the sun and moon were the first time that the sun and moon met. The sun and moon were the first time that the sun', 'Two thousand years ago, the vernal equinox was the first time that the sun and moon met, and the sun and moon were the first time that the sun and moon met.\\n\\nThe sun and moon were the first time that']\n"
     ]
    }
   ],
   "source": [
    "outputs=model_gpt.generate(**input, num_beams=2, num_return_sequences=2, max_length=50) #beam search\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Multinomial sampling**: sampling the token distribution\n",
    "- **Top-$K$ sampling**: sampling only $K$ samples with the highest probabilities.\n",
    "- **Nucleus sampling (top-$p$ sampling)**: Computing a cumulative distribution function and sampling only till the cut-off at the $p$ quantile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the iced tea industry had an average harvest of 1,000 tons of fresh tea per year. In the 1950s and 60s, we were drinking 4 to 5 tons of new tea per day, making the average daily']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the urns became the home of the ancient world by the rise of Prometheus. It became the center of the universe in some way or another.\\n\\nHow long has the earth been inhabited by men?\\n\\nThe']\n",
      "[\"Two thousand years ago, the vernacular of 'Lordhun' is a whole new weapon, as magic is reworked with a different twist, those ooze crystals grow larger to represent their wielder.\\n\\nTheir intricacies change and\"]\n"
     ]
    }
   ],
   "source": [
    "outputs=model_gpt.generate(**input, do_sample=True, max_length=50, early_stopping=True) # multinomial sampling\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))\n",
    "outputs=model_gpt.generate(**input, do_sample=True, top_k=50, max_length=50, early_stopping=True) # Top-k sampling\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))\n",
    "outputs=model_gpt.generate(**input, do_sample=True, top_p=0.90, top_k=0, max_length=50, early_stopping=True) #Top-p sampling\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Temperature**: A parameter which adjusts the logits before applying the softmax function. Greater temperature can lead to less probable words generated (more unexpected generated sequence), small temperature will lead to more conservative outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the vernacular was a rich language of the ancient world, which was combined with an advanced culture that was identifiable with the written language of the vernacular. The vernacular was a mixture of two different dialects']\n",
      "['Two thousand years ago, the vernacular of the ancient Egyptians was the language of the gods. The language of the gods was the language of the gods. The language of the gods was the language of the gods. The language of the gods was']\n"
     ]
    }
   ],
   "source": [
    "outputs=model_gpt.generate(**input, early_stopping=True, max_length=50, do_sample=True, top_k=0, temperature=0.7) #multinomial sampling with high temp\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))\n",
    "outputs=model_gpt.generate(**input, early_stopping=True, max_length=50, do_sample=True, top_k=0, temperature = 0.001) #multinomial sampling with low temp\n",
    "print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 üëÆ Logit Processors\n",
    "\n",
    "Logit processors can be applied to the computed logits, to alter them and therefore change the next most probable token. They are not dependant on any decoding strategy. Some of the basic usages are enforcing minimal or maximal length of the output, forbidding specific words etc. Here we will show you a naive approach to creating a Logit processor which enforces a token that should be in the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [8531], 'attention_mask': [1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Two thousand years ago, the vernacular of English was a mixture between \"the old\" and stupid. The word for it is literally translated as:\\nThe Old Man\\'s Wife (or Woman) [ edit ]\\n\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import LogitsProcessor, LogitsProcessorList, BatchEncoding\n",
    "import torch\n",
    "\n",
    "class EnforceWordProcessor(LogitsProcessor):\n",
    "    def __init__(self, desired_input: BatchEncoding, param: float = 5.5):\n",
    "        print(desired_input)\n",
    "        assert len(desired_input.input_ids) ==1\n",
    "        self.desired_input = desired_input.input_ids[0]\n",
    "        self.param = param #the parameter which indicates how much we want to enforce the word\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        last_id = input_ids[-1][-1] # ID of the last generated token (we are adjusting the next score)\n",
    "        if last_id != self.desired_input:\n",
    "            scores[-1,self.desired_input] = scores[-1,self.desired_input]*self.param\n",
    "        else:\n",
    "            scores[-1,self.desired_input] = -float(\"inf\") # we only want to generate the token once\n",
    "        return scores\n",
    "    \n",
    "enforced_word = \" stupid\" # we need the whitespace for the tokenizer to understand this as one token only\n",
    "\n",
    "custom_processor = EnforceWordProcessor(tokenizer_gpt(enforced_word, add_special_tokens=False))\n",
    "\n",
    "inputs = tokenizer_gpt(input_sentence, return_tensors=\"pt\")\n",
    "\n",
    "#repetition penalty is needed as the naive Logit Processor leads to repetition, so to avoid that, we include a rep. penalty\n",
    "# Here we are using a beam search decodeing strategy, which is much more likely to fall into the pit of repetition\n",
    "outputs = model_gpt.generate(**inputs, logits_processor=LogitsProcessorList([custom_processor]), max_length=140, repetition_penalty=3.0)\n",
    "output_text = tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "output_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are Logit Processors worth the work?\n",
    "\n",
    "The are use-cases, where a logit processor can help you enforce a rule of how structured the output should be. A really good example of that is the [**jsonformers**](https://github.com/1rgs/jsonformer) repo.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ‚úã Hands-on: Adjusting the output without retraining using Logit Processors\n",
    "\n",
    "In this first short session we would ask you, to take the previously shown naive Logit Processor and try to adjust it to perform better by:\n",
    "\n",
    "- enforce not a single token word, but a sequence of multiple tokens.\n",
    "- Try to fix the repetition problem without using a repetition penalty\n",
    "\n",
    "You can also experiment with different decoding strategies, which we talked about above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This is the start of the sequence.\\n\\nThe first thing to do is to create a new']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LogitsProcessor, LogitsProcessorList\n",
    "import torch\n",
    "\n",
    "class YourVeryOwnLogitProcessor(LogitsProcessor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        # The __call__ method has to return scores\n",
    "        return scores\n",
    "   \n",
    "custom_processor = YourVeryOwnLogitProcessor()\n",
    "\n",
    "text = \"This is the start of the sequence\"\n",
    "\n",
    "inputs = tokenizer_gpt(text, return_tensors=\"pt\") # tokenize the input sequence\n",
    "outputs = model_gpt.generate(**inputs, logits_processor=LogitsProcessorList([custom_processor])) # generate the output - now using greedy search\n",
    "output_text = tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True) # decode the output\n",
    "\n",
    "output_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Hands-on] solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Well, I hope you had a great ML Prague conference.\\nI'm sure there are many more that will be coming soon!\"]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LogitsProcessor, LogitsProcessorList, BatchEncoding\n",
    "import torch\n",
    "from numpy import sign, log\n",
    "\n",
    "class EnforceTokenSequenceProcessor(LogitsProcessor):\n",
    "    def __init__(self, desired_input: BatchEncoding, input: BatchEncoding, param: float =1.1):\n",
    "        self.start_len = len(input.input_ids) # Length of the original input\n",
    "        self.desired_input = desired_input.input_ids # Sequence of tokens we want to generate\n",
    "        self.desired_input_len = len(self.desired_input) # Length of the desired sequence\n",
    "        self.param = param # Parameter which indicates how much we want to enforce the sequence (trunctuated to the value of 1 if <1)\n",
    "        self.num_gen_tokens = 0 # Index on how many tokens from the sequence are generated\n",
    "        self.blacklist = [] # A list of tokens from the desired sequence which have already been generated\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        last_id = input_ids[-1][-1] # ID of the last generated token (we are adjusting the next score)\n",
    "        gen_input_length = input_ids.shape[1]-self.start_len # length of generated text AFTER the input text\n",
    "\n",
    "        # the exponent is for enforcing the sequence more, the longer the token is not generated\n",
    "        # logarithm function because it is a lower growing function but\n",
    "        # we have to trunctuate the function for param < 1 \n",
    "        enforcement_param = log(self.param**gen_input_length) if not log(self.param**gen_input_length) < 1 else 1\n",
    "\n",
    "        # If we are generating the sequence we move to the next token and add the generate token to blacklist\n",
    "        if last_id == self.desired_input[self.num_gen_tokens]:\n",
    "            self.blacklist.append(self.desired_input[self.num_gen_tokens])\n",
    "            if self.num_gen_tokens < self.desired_input_len-1:\n",
    "                self.num_gen_tokens+=1\n",
    "\n",
    "        # If we stopped generating sequence not because of completing it we reset and start enforcing all over\n",
    "        elif last_id != self.desired_input[self.num_gen_tokens] and self.num_gen_tokens > 0 and self.num_gen_tokens+1 != self.desired_input_len:\n",
    "            self.blacklist = []\n",
    "            self.num_gen_tokens = 0\n",
    "\n",
    "        # Adjustment of scores\n",
    "\n",
    "        # Rise the scores of the token we want to generate\n",
    "        # sign function make sure we multiply scores > 0 and divide scores < 0 - to always increase the score \n",
    "        scores[-1,self.desired_input[self.num_gen_tokens]] = scores[-1,self.desired_input[self.num_gen_tokens]]*enforcement_param**sign(scores[-1,self.desired_input[self.num_gen_tokens]])\n",
    "        # Make sure the previous tokens from the sequence cannot be generated\n",
    "        scores[-1,self.blacklist] = -float(\"inf\")\n",
    "        return scores\n",
    "    \n",
    "enforced_sentence = \" hope you had a great ML Prague conference\"\n",
    "input_sentence = \"Well,\"\n",
    "\n",
    "inputs = tokenizer_gpt(input_sentence, return_tensors=\"pt\")\n",
    "tokenized_enforced_sequence = tokenizer_gpt(enforced_sentence, add_special_tokens=False)\n",
    "\n",
    "# Initialize tour Logit Processor\n",
    "custom_processor = EnforceTokenSequenceProcessor(tokenized_enforced_sequence, inputs, param = 2.1)\n",
    "\n",
    "#repetition penalty is needed as the naive Logit Processor leads to repetition, so to avoid that, we include a rep. penalty\n",
    "outputs = model_gpt.generate(**inputs, logits_processor=LogitsProcessorList([custom_processor]),max_length=140, repetition_penalty=3.0) #greedy search\n",
    "output_text = tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "output_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the way of enforcing the given sentence is not really an elegant way. Also, if you try using it with beam search, you will find, that nothing will be generated, as there are other sequences whose joined probability is higher.\n",
    "\n",
    "If you want to use beam search as the decoding strategy, and are interested in forced words generation try [**constrained beam search**](https://huggingface.co/blog/constrained-beam-search)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
