{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n",
      "Requirement already satisfied: wheel in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentencepiece in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (0.1.99)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentence-transformers in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sentence-transformers) (0.14.1)\n",
      "Requirement already satisfied: nltk in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: tqdm in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sentence-transformers) (4.29.2)\n",
      "Requirement already satisfied: filelock in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\n",
      "Requirement already satisfied: fsspec in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.5.0)\n",
      "Requirement already satisfied: requests in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.30.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (59.6.0)\n",
      "Requirement already satisfied: wheel in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (0.40.0)\n",
      "Requirement already satisfied: cmake in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: click in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install sentencepiece\n",
    "!{sys.executable} -m pip install sentence-transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "m_path = \"t5-small\"#\"bert-base-uncased\"\n",
    "# model = AutoModel.from_pretrained(m_path)\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sum', '##mar', '##ize', ':', 'this', 'is', 'an', 'input', 'text', '##w', '##hic', '##h', 'is', 'b', '##lab', '##lab', '##la', '.', '[SEP]']\n",
      "[101, 7680, 7849, 4697, 1024, 2023, 2003, 2019, 7953, 3793, 2860, 16066, 2232, 2003, 1038, 20470, 20470, 2721, 1012, 102, 102]\n",
      "['[CLS]', 'sum', '##mar', '##ize', ':', 'this', 'is', 'an', 'input', 'text', '##w', '##hic', '##h', 'is', 'b', '##lab', '##lab', '##la', '.', '[SEP]', '[SEP]']\n",
      "{'input_ids': [101, 7680, 7849, 4697, 1024, 2023, 2003, 2019, 7953, 3793, 2860, 16066, 2232, 2003, 1038, 20470, 20470, 2721, 1012, 102, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"summarize: This is an input textwhich is blablabla. \\n [SEP] \"\n",
    "input_tokenized = tokenizer_bert.tokenize(input_sentence)\n",
    "print(input_tokenized)\n",
    "input_tokenized2 = tokenizer_bert.encode(input_sentence)\n",
    "print(input_tokenized2)\n",
    "text_token=tokenizer_bert.convert_ids_to_tokens(input_tokenized2)\n",
    "print(text_token)\n",
    "print(tokenizer_bert(input_sentence))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output: This is the supercalifragilisticexpialidocious input text.\n",
      "Tokenized output (in IDs): [100, 19, 8, 1355, 15534, 20791, 173, 3040, 994, 102, 23, 4288, 7171, 2936, 3785, 1499, 5, 1]\n",
      "Tokenized output (in tokens): ['▁This', '▁is', '▁the', '▁super', 'cali', 'frag', 'il', 'istic', 'ex', 'p', 'i', 'ali', 'doc', 'ious', '▁input', '▁text', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"This is the supercalifragilisticexpialidocious input text.\"\n",
    "tokenized_input = tokenizer_t5(input_sentence)\n",
    "\n",
    "print(\"Original output: {}\".format(input_sentence))\n",
    "print(\"Tokenized output (in IDs): {}\".format(tokenized_input.input_ids))\n",
    "print(\"Tokenized output (in tokens): {}\".format(tokenizer_t5.convert_ids_to_tokens(tokenized_input.input_ids)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "First we need to transform our tokenized input into a vector representation. This representantion needs to hold information about the semantic property of a word (*Input Embedding*) and a positional information about where the word is in the input sequence (*Positional Embedding*). Each word's embedding has a size of a hidden layer (default 512). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the input ids tensor: torch.Size([1, 18])\n",
      "Size of the word embedding for the input sequence: torch.Size([1, 18, 512])\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(m_path)\n",
    "input_ids = tokenizer_t5(input_sentence, return_tensors=\"pt\")\n",
    "outputs=model.generate(input_ids.input_ids, return_dict_in_generate=True,output_hidden_states=True, output_attentions=True, output_scores=True)\n",
    "\n",
    "word_embeddings_for_input_sentence = outputs.encoder_hidden_states[0] \n",
    "print(\"Size of the input ids tensor: {}\".format(input_ids.input_ids.shape))\n",
    "print(\"Size of the word embedding for the input sequence: {}\".format(word_embeddings_for_input_sentence.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What does Self-attention do?\n",
    "Self-attention layer is a building block of the Transformer infrastructure, which is able to include the information about the connection between  individual words/tokens from the input sequence. \n",
    "\n",
    "The image bellow shows from which the computation of self-attention consists. Each of the listed matrices (Q, K and V) is computed by multiplication of the matrix of input embeddings and a weight matrices ($W^{Q}$, $W^{K}$, $W^{V}$). These matrices contain the parameters which are trained during training. \n",
    "\n",
    "![image.png](https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n",
    "[[source]](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "Each self-attention head has its own weight matrices ($W^{Q}$, $W^{K}$, $W^{V}$), which are trained independently of each other.\n",
    "\n",
    "The aim of the attention layer is to provide the model with understanding the relevancy of the rest of the tokens to the token being processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 18, 18])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.encoder_attentions[1].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Full Architecture\n",
    "![animation.gif](https://heidloff.net/assets/img/2023/02/transformers.png)\n",
    "[[source]](https://heidloff.net/article/foundation-models-transformers-bert-and-gpt/)\n",
    "\n",
    "![animation.gif](https://jalammar.github.io/images/t/transformer_decoding_2.gif)\n",
    "[[source]](https://jalammar.github.io/illustrated-transformer/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks / Multiple objectives\n",
    "As stated above Transformers can be used in multiple ways, depending on the desired output is. We can either use the whole architecture as shown above or use only the decoder or encoder. In every task the input is sequence of tokens. However the tasks differ depending on if they use the decoder or not. When using only the encoder t the output is either a label (text clf) a label for each token (token clf) or 2 indices which border the span of extracted answer. However when we use the decoder we then iteratively predict a single token (until a special token is generated, which symbolizes the end of the sentence.)\n",
    "\n",
    "### Using Encoder only\n",
    "The encoder hidden state provides contextual embeddings which can then be transformed to perform multiple tasks. The input is sequence where the output is either a label (text clf) a label for each token (token clf) or 2 indices which border the span of extracted answer\n",
    "Heads:\n",
    "* Text classification (positive/negative): The head is a linear layer processes the encoders final hidden states and transformes them into logits (which are then used as probabilities to classify the sentence)\n",
    "\n",
    "* Token classification (NER): linear layer processes the hidden states to logits, where cross-entropy loss is computed, to return a most probable label for each token\n",
    "\n",
    "* Extractive QA: linear layer processes to the start and end logits,  cross-entropy loss -> find best candidates for start and end of the answer in sequence\n",
    "\n",
    "### Using Decoder only\n",
    "* Text generation: The input sequence embeddings are parsed straight into the decoder, which then generates the most probable following tokens (the last hidden state of the decoder is processed by a linear layer and once more we get logits from which we generate the most probable token.\n",
    "\n",
    "### Encoder-Decoder\n",
    "* Sequence to sequence (Summarization, Translation): These tasks use the encoder and the decoder to process the input sequence. It is a text generation task (as discused above), however instead of continuing to generate based on an input prompt we also have the encoder to provide the contextual information of the input sequence, which we then then process. Both of these tasks are mainly about transcribing the input into a different text (translate to a different language / simplify the text but maintain the meaning)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training & Fine-tuning\n",
    "Pre-training and Fine-tuning\n",
    "\n",
    "The magic of Deep language models lays in their good preliminary knowledge of language. They obtain this knowledge during so-called pre-training phase, where they are trained for a different token classification task an instance of Langauge Modeling.\n",
    "\n",
    "\n",
    "### Language Modeling\n",
    "\n",
    "In the instances of Language Modeling task, models are asked to solve a task of 'guessing' the **right word in the context**.\n",
    "\n",
    "This task comes in two main instances:\n",
    "\n",
    "1. **Masked Language Modeling (MLM)**: Models guess the correct token **within context**. This objective best prepares the model for **classification tasks** (Named Entity Recognition, Sequence Classification)\n",
    "\n",
    "![image.png](https://www.rohanawhad.com/content/images/size/w1600/2022/04/image.png)\n",
    "[[source]](https://www.rohanawhad.com/improvements-of-spanbert-over-bert/)\n",
    "\n",
    "2. **Causal Langauge Modeling (CLM)**: Models guess the **following token** from previous context. This objective is better for preparing the model for **generation**, such as Dialogue, Summarization, Translation.\n",
    "\n",
    "![image.png](https://gcdnb.pbrd.co/images/Bx4h6Lordx0y.png?o=1)  \n",
    "![image.png](https://gcdnb.pbrd.co/images/rb7bmZS11gtl.png?o=1)\n",
    "![image.png](https://gcdnb.pbrd.co/images/gXYffjzLIk7n.png?o=1)\n",
    "\n",
    "Other variances: Synthetic token classification (ALBERT), Mask infilling (BART, T5), Sequence reconstruction (BART), Sequence Classification (BERT).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n",
    "Even though it may seem, that once we have a trained model we are done, there is the generation itself. While the weights in the decoder (and encoder) are set, we can still tweak the generated output, by using multiple generation strategies as well as using Logit Processors, which alter the logits from the linear layer and therefore alter the mostprobable next token. \n",
    "\n",
    "### What are Logits Processors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dies is the supercali']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LogitsProcessor, LogitsProcessorList\n",
    "import torch\n",
    "\n",
    "class TestProcessor(LogitsProcessor):\n",
    "    def __init__(self, max_length:int, eos_token_id: int):\n",
    "        self.max_length = max_length\n",
    "        self.eos_token_id = eos_token_id\n",
    "        pass\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        if input_ids.shape[1] > self.max_length:\n",
    "            scores[-1,:] = -float(\"inf\")\n",
    "            scores[-1, self.eos_token_id] = 0\n",
    "        return scores\n",
    "\n",
    "custom_processor = TestProcessor(5, tokenizer_t5.eos_token_id)\n",
    "\n",
    "inputs = tokenizer_t5(input_sentence, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, logits_processor=LogitsProcessorList([custom_processor]))\n",
    "output_text = tokenizer_t5.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can you create with Logits Processors?\n",
    "\n",
    "A really good example of working with Logit processors is the jsonformers (https://github.com/1rgs/jsonformer) repo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding strategies\n",
    "While iteratively generating the output tokens, there are multiple strategies, that can be used to achieve the \"best\" output text.\n",
    "* Greedy search: At each iteration, the most probable token is generated (most probable token at a time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 1.34MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 548M/548M [00:13<00:00, 40.2MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 432kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.67MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 40.3MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 5.22MB/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the first humans were living in the world. Today, they are the only living species on Earth.\\n\\nThe first humans were living in the world. Today, they are the only living species on Earth. The first humans']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "model_path = \"gpt2\"\n",
    "input_sentence = \"Two thousand years ago\"\n",
    "model_2 = AutoModelWithLMHead.from_pretrained(model_path)\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_path)\n",
    "input_ids = tokenizer_2(input_sentence, return_tensors=\"pt\")\n",
    "outputs=model_2.generate(**input_ids, early_stopping=True, max_length=50)\n",
    "print(tokenizer_2.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Beam search: At each iteration $N$ number of beams with the best overall probability are stored. After the generation is complete the beam with highes overall probability is returned (most probable output as a whole)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Two thousand years ago, a group of scientists from the University of California, Berkeley, and the University of California, Santa Barbara, developed a method to measure the chemical composition of the Earth's crust. The team found that the Earth's crust is composed\", \"Two thousand years ago, a group of scientists from the University of California, Berkeley, and the University of California, Santa Barbara, developed a method to measure the chemical composition of the Earth's crust. The team found that the Earth's crust was composed\"]\n"
     ]
    }
   ],
   "source": [
    "outputs=model_2.generate(input_ids.input_ids, num_beams=2, num_return_sequences=2, max_length=50)\n",
    "print(tokenizer_2.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Temperature: A parameter which adjusts the logits before applying the softmax function. Greater temperature can lead to less probable words generated (more unexpected generated sequence), small temperature will lead to more conservative outputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, the Arabs had been the most advanced people in the world. The slaves and the cattle of the Arabs had been domesticated. The Israelites were the first people in the world to produce a foodstuff that was not only dense']\n",
      "['Two thousand years ago, the first humans were living in the world. Today, they are the only living species on Earth.\\n\\nThe first humans were living in the world. Today, they are the only living species on Earth. The first humans']\n"
     ]
    }
   ],
   "source": [
    "outputs=model_2.generate(**input_ids, early_stopping=True, max_length=50, do_sample=True, top_k=0, temperature=0.7)\n",
    "print(tokenizer_2.batch_decode(outputs, skip_special_tokens=True))\n",
    "outputs=model_2.generate(**input_ids, early_stopping=True, max_length=50, do_sample=True, top_k=0, temperature = 0.001)\n",
    "print(tokenizer_2.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Top-$K$ sampling: sampling only $K$ samples with the highest probabilities.  \n",
    "* Nucleus sampling (top-$p$ sampling): Computing a cumulative distribution function and sampling only till the cut-off at the $p$ quantile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two thousand years ago, for a while, you would have thought our history would lie far back in the Jurassic past.\\n\\nBut the dinosaurs were really out from under the sun, so a great deal had just been lost.\\n\\nOne story']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Two thousand years ago, the sun was formed and slowly, but slowly began to set in space, and we see it every time this sun sets (that's what we all assumed).\\n\\nOne of the most common misconceptions about the solar system is\"]\n",
      "['Two thousand years ago, the first humans were the first to colonize the planet. Today, the first humans are the first to colonize the planet.\\n\\nThe first humans were the first to colonize the planet. Today, the first humans']\n"
     ]
    }
   ],
   "source": [
    "outputs=model_2.generate(input_ids.input_ids, do_sample=True, max_length=50, early_stopping=True)\n",
    "print(tokenizer_2.batch_decode(outputs, skip_special_tokens=True))\n",
    "outputs=model_2.generate(input_ids.input_ids, do_sample=True, top_k=50, max_length=50, early_stopping=True)\n",
    "print(tokenizer_2.batch_decode(outputs, skip_special_tokens=True))\n",
    "outputs=model_2.generate(input_ids.input_ids, do_sample=True, top_p=0.1, max_length=50, early_stopping=True)\n",
    "print(tokenizer_2.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on: Adjusting the output without retraining\n",
    "\n",
    "In this first short session we would ask you, to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The soldiers were a army of the troops who had been killed in their wars and du']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, PhrasalConstraint\n",
    "# model_path = \"distilgpt2\"\n",
    "input_sentence =  \"translate English to German: How old are you?\"\n",
    "input_sentence = \"The soldiers \\n \\n\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "constraints = [\n",
    "    PhrasalConstraint(\n",
    "        tokenizer(\"du\", add_special_tokens=False).input_ids\n",
    "    )\n",
    "]\n",
    "input_ids = tokenizer(input_sentence, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    input_ids.input_ids,\n",
    "    constraints=constraints,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "# outputs=model.generate(input_ids.input_ids, do_sample=True, top_k=50, max_length=50, early_stopping=True)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁hit', 'ler', '</s>']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer(\"hitler\").input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
