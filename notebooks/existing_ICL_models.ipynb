{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqjVhkOBaORvcFasrpiR7r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaussalgo/L2L_MLPrague23/blob/main/notebooks/existing_ICL_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview of existing smaller ICL models\n",
        "\n",
        "Despite the ever-growing popularity of OpenAI's GPT-based models, it is less known that there exists a range of smaller in-context learners that compare and outperform OpenAI's GPT3 on learning unseen task(s) from input context.\n",
        "\n",
        "Below is a non-exhaustive list of smaller ICL models. For each, we take a look at how the training format of the inputs and labels looked for the particular model."
      ],
      "metadata": {
        "id": "0Of7mv-HUem_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main categories\n",
        "\n",
        "\n",
        "* Foundation models\n",
        "* Instruction following (T0, Tk-Instruct, LLama/Alpaca, MPT)\n",
        "* Chain-of-Thought generation (FLAN)\n",
        "* Few-shot tuning, possibly combined with others (Tk-Instruct, FLAN)"
      ],
      "metadata": {
        "id": "jvPo0YxmWv71"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32614ce5"
      },
      "source": [
        "### List of models\n",
        "\n",
        "* T5 models (https://huggingface.co/t5-large) - pre-trained on mixture of tasks in text2text format\n",
        "* mT5 models (https://huggingface.co/google/mt5-large) - pre-trained on 101 languages, no supervised tasks - needs to be fine-tuned\n",
        "* Tk models (https://huggingface.co/allenai/tk-instruct-large-def-pos) - fine-tuned T5 on tasks with prompts written as in-context instructions\n",
        "* mTk models (https://huggingface.co/allenai/mtk-instruct-3b-def-pos) - multilingual version of the Tk model\n",
        "* FLAN - T5 models (https://huggingface.co/google/flan-t5-large) - fine-tuned T5 model on 1000 additional tasks, including Chain-of-Thought\n",
        "* Alpaca (https://github.com/tatsu-lab/stanford_alpaca)\n",
        "* Dolly (https://huggingface.co/databricks/dolly-v2-7b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### T5/mT5\n",
        "\n",
        "Foundation models pre-trained for a generation of fill-in segments of texts, that have been shown to be very efficient in a fine-tuning for instruction following. While the original T5 was also fine-tuned on five tasks (including generation, classification and question answering), recently it is advised to instead use the [non-finetuned version](https://huggingface.co/google/t5-v1_1-large) also for the fine-tuning experiments."
      ],
      "metadata": {
        "id": "XW9qhjWcgpbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "This is [MASK] text where the model is trained to correctly predict a sequence of the original tokens .\n",
        "\"\"\"\n",
        "\n",
        "label = \"a piece of\""
      ],
      "metadata": {
        "id": "6-jY8FxWiD2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### T0 (T-Zero)\n",
        "\n",
        "3-billion and 11-billion models fine-tuned from T5 on a mixture of 35 tasks with all templates available in Promptsource. Large portion of these tasks are question answering tasks"
      ],
      "metadata": {
        "id": "hyN5kG8HXyd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# promptsource template from:\n",
        "# https://github.com/bigscience-workshop/promptsource/blob/main/promptsource/templates/adversarial_qa/adversarialQA/templates.yaml\n",
        "\n",
        "input_text = \"\"\"\n",
        "\n",
        "Extract the answer to the question from the following context.\n",
        "\n",
        "Question: Which happened earlier, the Fair District Amendments to the state constitution was passed or the legislature's redistricting was announced?\n",
        "\n",
        "Context: Reapportionment following the 2010 United States Census gave the state two more seats in the House of Representatives. The legislature's redistricting, announced in 2012, was quickly challenged in court, on the grounds that it had unfairly benefited Republican interests. In 2015, the Florida Supreme Court ruled on appeal that the congressional districts had to be redrawn because of the legislature's violation of the Fair District Amendments to the state constitution passed in 2010; it accepted a new map in early December 2015.\n",
        "\"\"\"\n",
        "\n",
        "label = \"the Fair District Amendments to the state constitution passed\""
      ],
      "metadata": {
        "id": "KnPTBdoiXvGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tk-Instruct\n",
        "\n",
        "Instruction model fine-tuned explcitly for few-shot learning: Input texts first contained an instruction, followed by a list of examples. Generalization to unseen tasks is expected to emerge from a **vast volume of 1000+ diverse training tasks**.\n",
        "\n",
        "This ICL model also comes in two **multilingual versions** (3B and 11B params), additionally trained on 576 non-English tasks (but preceded with English instructions) in 55 languages.\n",
        "\n",
        "This model is trained on a different collection of seq2seq tasks in [NaturalInstructions](https://github.com/allenai/natural-instructions)."
      ],
      "metadata": {
        "id": "UwVR4HBKZaPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "\n",
        "Given a tweet, classify it into one of 4 categories: Positive, Negative, Neutral, or Mixed.\n",
        "\n",
        "Input: I love dark chocolate.\n",
        "Output: Positive\n",
        "\n",
        "Input: I will neither stop by nor use your product from now on\n",
        "Output: Negative\n",
        "\n",
        "Input: I am boycotting all Ãœlker products from now on\n",
        "Output:\n",
        "\"\"\"\n",
        "\n",
        "label = \"Negative\""
      ],
      "metadata": {
        "id": "-U_waBxpZ7fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FLAN\n",
        "\n",
        "In addition to NaturalInstructions dataset used by Tk-instruct, FLAN models are also trained for prediction of **Chain-of-Thought (CoT) sequences**; reasoning chains that link the input prompt to the generated output.\n",
        "\n",
        "FLAN was fine-tuned with a combination of both zero-shot and few-shot input prompts, so contrary to Tk-Instruct, it makes sense to use it both with, and without demonstrations."
      ],
      "metadata": {
        "id": "pjsRdaRNSve8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "\n",
        "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. \n",
        "How many clips did Natalia sell altogether in April and May?\n",
        "\"\"\"\n",
        "\n",
        "label = \"Natalia sold 48/2 = 24 clips in May. Natalia sold 48+24 = 72 clips altogether in April and May. The answer is 72.\""
      ],
      "metadata": {
        "id": "foYy2KXxSve9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alpaca/Dolly models\n",
        "\n",
        "Instruction-following models ranging from 7B to 64B parameters. The ground-truth labels for the newly-collected input instructios were collected as the outputs of ChatGPT, which is why the cannonical version of Alpaca can not be used for commercial purposes (but its followup models, like Dolly can!)"
      ],
      "metadata": {
        "id": "54PvIYBwdFPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "\n",
        "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Give three tips for staying healthy.\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "label = \"1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. 2. Exercise regularly to keep your body active and strong. 3. Get enough sleep and maintain a consistent sleep schedule.\""
      ],
      "metadata": {
        "id": "tyVJ0ZDCezzU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}