{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaussalgo/L2L_MLPrague23/blob/main/notebooks/hands_on_improving_ICL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9f30c1a-8962-4236-b7aa-3718d08548da"
      },
      "source": [
        "# Training generative models\n",
        "\n",
        "Now that we have a theoretical background, we'll take a look at how the covered generative models are actually trained. \n",
        "\n",
        "After the overview, we will finally utilize this knowledge in training our own in-context learner, taylored for a new language, or improved on a specific task of your interest!\n"
      ],
      "id": "c9f30c1a-8962-4236-b7aa-3718d08548da"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "42d0299e-e0b4-4e36-8b1d-2322b68cab54"
      },
      "outputs": [],
      "source": [
        "# notebook's requirements\n",
        "!pip install -q adaptor transformers datasets"
      ],
      "id": "42d0299e-e0b4-4e36-8b1d-2322b68cab54"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b487bae2-bbf2-44c4-9ea6-70126e319792"
      },
      "source": [
        "## Generation - refresher\n",
        "\n",
        "Recall the **Causal Langauge Modeling (CLM)** from earlier, where the model predicts the **following token** from previous context.\n",
        "\n",
        "![CLM_1](https://github.com/gaussalgo/L2L_MLPrague23/blob/main/notebooks/images/CLM_1_new.png?raw=1)  \n",
        "![CLM_2](https://github.com/gaussalgo/L2L_MLPrague23/blob/main/notebooks/images/CLM_2_new.png?raw=1)\n",
        "![CLM_3](https://github.com/gaussalgo/L2L_MLPrague23/blob/main/notebooks/images/CLM_3_new.png?raw=1)\n",
        "\n",
        "[[images source]](https://www.rohanawhad.com/improvements-of-spanbert-over-bert/)\n",
        "\n",
        "Note the relation of *generation* and CLM objective: We predict the next token conditionally to the input. Only in the case of \"real\" generation, the **input also contains the previous outputs** of the model."
      ],
      "id": "b487bae2-bbf2-44c4-9ea6-70126e319792"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ab407ee-eedc-4843-9d58-a372ce76b86a"
      },
      "source": [
        "## Construction of Training pipeline\n",
        "\n",
        "Many libraries makes it easy to train your NN model, with different levels of user complexity - do not get confused by that.\n",
        "\n",
        "Ordered by implementation complexity incrementally, for PyTorch models, you can find at least these: **Pure PyTorch, PyTorch Lightning, Transformers Trainer, Adaptor (ours)**. We will take a look at how it looks at the most low-level (Pure PyTorch), and the most high-level (Adaptor), but in your time, you can also look at [Sequence Classification tutorial](https://huggingface.co/docs/transformers/tasks/sequence_classification) from HuggingFace.\n"
      ],
      "id": "6ab407ee-eedc-4843-9d58-a372ce76b86a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95be66f8-ce1c-43a5-a49d-91832e69541c"
      },
      "source": [
        "## â›µ Low-level Training pipeline - Example\n",
        "\n",
        "Here, we are going to take a look at the low level of updating a network.\n",
        "The process can be summarized in the following steps:\n",
        "\n",
        "1. **We pick our base model** to fine-tune. While all Transformer LMs can perform token classification, not all of them are equally good at it. We'll talk about it a bit more offline.\n",
        "\n",
        "2. **We construct training dataset** from our data. Here, we transform texts into valid model inputs/samples (as we've seen in previous session) and assign true labels for each sample.\n",
        "\n",
        "3. **We iterate over the samples** in so-called *epochs*. In this step, we get the model predictions for a *batch* of samples: The raw predictions take form of probabilities (usually log-probabilities, to make prediction faster).\n",
        "\n",
        "4. **We update the model**. Here, we first compare the predicted probabilities with \"true probabilities\", where true category gets a probability==1, and other categories get 0. The comparison is done by so-called *loss* function, which is a special version of distance measure. Then, we update the weights of the model so that they improve the loss metric.\n",
        "\n",
        "5. **We continue as long as the model improves**, which we measure on a held-out dataset, to avoid that the model just learns to remember our data (but then would perform badly in the real world)."
      ],
      "id": "95be66f8-ce1c-43a5-a49d-91832e69541c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE7HFMhTExJ0"
      },
      "source": [
        "### 1: Pick our base model"
      ],
      "id": "aE7HFMhTExJ0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe3494eb-b0c4-4756-b391-4277eb537e03"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': 'pad'})"
      ],
      "id": "fe3494eb-b0c4-4756-b391-4277eb537e03"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSJXMP6KEq-F"
      },
      "source": [
        "### 2: Constructing training dataset"
      ],
      "id": "FSJXMP6KEq-F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84e44004-d65f-41a3-8dbf-1964e0d3ddd5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"imdb\")"
      ],
      "id": "84e44004-d65f-41a3-8dbf-1964e0d3ddd5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JxXF_RVYUmR"
      },
      "source": [
        "### Dataset transformations\n",
        "\n",
        "Before we jump into the training routine, we'll zoom in on data processing we need for training GPT-like Causal language models.\n",
        "\n",
        "First, we'll transform the sample on model's input ids using the associated tokenizer. Then, we'll take a look at labels construction."
      ],
      "id": "7JxXF_RVYUmR"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b-ZgwraRh5SV"
      },
      "outputs": [],
      "source": [
        "sample_encoding = tokenizer(dataset[\"unsupervised\"]['text'][10], \n",
        "                            padding=\"longest\",  # we'll set padding and truncation so that tokenizer allows us to directly obtain tensors\n",
        "                            truncation=True,\n",
        "                            return_tensors=\"pt\"  # this returns the samples as PyTorch tensors, that we do not have to convert ourselves\n",
        "                            )"
      ],
      "id": "b-ZgwraRh5SV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx33FdQeE5yH"
      },
      "source": [
        "#### Constructing training labels"
      ],
      "id": "bx33FdQeE5yH"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "xteHowKaD7aW",
        "outputId": "5cd73ac7-1bb8-425a-92f5-227c2ddd8441"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This| isn|'t| the| worst| comedy| of| all|-|time|,| but| that| is| about| the| best| thing| that| I| can| say| about| this| pathetic| film|.| I| didn|'t| laugh| once|,| or| even| smile| once| during| this| bomb|.| There| was| usually| something| going| on| on|-|screen|,| so| I| didn|'t| get| TO|O| bored|,| but| most| of| the| jokes| here| were| simply| awful|.| The| final| sequence| is| nothing| more| than| a| long| series| of| people| falling| through| doors| and| stumbling| all| over| the| place|.| Needless| to| say|,| it| was| a| fitting| way| to| end| a| movie| that| was| impossible| for| me| to| like|.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# GPT's input ids, decoded:\n",
        "\"|\".join(tokenizer.batch_decode(sample_encoding[\"input_ids\"][0]))"
      ],
      "id": "xteHowKaD7aW"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "5v2Mb3FuDqBX",
        "outputId": "cc32cdf4-4e92-4ac5-b986-e8ec4c395be7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" isn|'t| the| worst| comedy| of| all|-|time|,| but| that| is| about| the| best| thing| that| I| can| say| about| this| pathetic| film|.| I| didn|'t| laugh| once|,| or| even| smile| once| during| this| bomb|.| There| was| usually| something| going| on| on|-|screen|,| so| I| didn|'t| get| TO|O| bored|,| but| most| of| the| jokes| here| were| simply| awful|.| The| final| sequence| is| nothing| more| than| a| long| series| of| people| falling| through| doors| and| stumbling| all| over| the| place|.| Needless| to| say|,| it| was| a| fitting| way| to| end| a| movie| that| was| impossible| for| me| to| like|.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# GPT's training labels: when training to predict the following token, we simply shift the inputs one position to the right\n",
        "labels = sample_encoding[\"input_ids\"][..., 1:]\n",
        "\n",
        "\"|\".join(tokenizer.batch_decode(labels[0]))"
      ],
      "id": "5v2Mb3FuDqBX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFxPgvIvNl9T"
      },
      "source": [
        "#### Creating next-token prediction inputs from each sample\n",
        "\n",
        "For the next-token prediction, we actually create multiple samples from each text: There are **many** tokens that we can use as **targets**!\n",
        "\n",
        "To make it easier for us, we'll repeatedly use the **same input ids**, and only **attend to the previous tokens**, to be used in prediction. We just need to be careful not to un-mask the actually-predicted token.\n",
        "\n",
        "We can implement this quite easily by constructing a **triangular attention mask** for each input from the batch."
      ],
      "id": "aFxPgvIvNl9T"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANSt04a0Nrv7",
        "outputId": "2d6d74b2-f209-4d94-ef97-e46463300348"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([113])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "attended_input_length = sample_encoding[\"attention_mask\"].sum(axis=1)\n",
        "attended_input_length"
      ],
      "id": "ANSt04a0Nrv7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL4S3SOEiJhO"
      },
      "source": [
        "**Input ids**: duplicate inputs by the number of predicted tokens"
      ],
      "id": "eL4S3SOEiJhO"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bpxWhHQQAUh",
        "outputId": "bcb5892b-a3ff-467f-a849-1f269fbd287a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1212, 2125,  470,  ...,  284,  588,   13],\n",
              "        [1212, 2125,  470,  ...,  284,  588,   13],\n",
              "        [1212, 2125,  470,  ...,  284,  588,   13],\n",
              "        ...,\n",
              "        [1212, 2125,  470,  ...,  284,  588,   13],\n",
              "        [1212, 2125,  470,  ...,  284,  588,   13],\n",
              "        [1212, 2125,  470,  ...,  284,  588,   13]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# duplicate inputs by the number of predicted tokens\n",
        "input_ids = sample_encoding[\"input_ids\"].expand(attended_input_length, -1)\n",
        "input_ids"
      ],
      "id": "0bpxWhHQQAUh"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbKQ5_5fnNMJ",
        "outputId": "eb12cf38-ab8b-4e23-8fbe-ea560b026195"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([113, 113])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "input_ids.shape"
      ],
      "id": "RbKQ5_5fnNMJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bKQm1DyiLSU"
      },
      "source": [
        "**Attention mask**: we create triangles that will mask all future tokens from prediction"
      ],
      "id": "5bKQm1DyiLSU"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEA8oIf7iYMr",
        "outputId": "27e916da-fae6-4f0a-dcfd-d59168afc5f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import torch \n",
        "\n",
        "# this is how we construct tensor triangles\n",
        "torch.tril(torch.ones(4, 6), diagonal=0)"
      ],
      "id": "uEA8oIf7iYMr"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQvzkxXLiLxk",
        "outputId": "c2e6cc94-3303-49b9-a09f-ee93e561686b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "attention_triangle = torch.tril(torch.ones(attended_input_length, input_ids.shape[1]), diagonal=0)\n",
        "attention_triangle"
      ],
      "id": "HQvzkxXLiLxk"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFyZ1L2znMHP",
        "outputId": "4b8bacb7-3e01-487e-bfbc-ff5f27d5eb10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([113, 113])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "attention_triangle.shape"
      ],
      "id": "rFyZ1L2znMHP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6HZwI09iVGD"
      },
      "source": [
        "**Labels**: Finally, we spread the pre-computed labels to assign exactly one label id to each new sample"
      ],
      "id": "e6HZwI09iVGD"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "VJrQS0Z1iVmi",
        "outputId": "1260a055-e499-4d02-ccc8-13d1fc20a4f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" isn|'t| the| worst| comedy| of| all|-|time|,| but| that| is| about| the| best| thing| that| I| can| say| about| this| pathetic| film|.| I| didn|'t| laugh| once|,| or| even| smile| once| during| this| bomb|.| There| was| usually| something| going| on| on|-|screen|,| so| I| didn|'t| get| TO|O| bored|,| but| most| of| the| jokes| here| were| simply| awful|.| The| final| sequence| is| nothing| more| than| a| long| series| of| people| falling| through| doors| and| stumbling| all| over| the| place|.| Needless| to| say|,| it| was| a| fitting| way| to| end| a| movie| that| was| impossible| for| me| to| like|.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "labels = sample_encoding[\"input_ids\"][..., 1:][:attended_input_length]\n",
        "labels\n",
        "\"|\".join(tokenizer.batch_decode(labels[0]))"
      ],
      "id": "VJrQS0Z1iVmi"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARYScE-vnq9d",
        "outputId": "7bac9ea7-b4b0-4f99-cc6a-89edc716bb73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# After the model is done, we want it to generate a special <EoS> token. This way, we know that the model is done with generation.\n",
        "model.config.eos_token_id"
      ],
      "id": "ARYScE-vnq9d"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KECoWaGUocoh"
      },
      "outputs": [],
      "source": [
        "# Hence, we add the token as the last label\n",
        "labels = torch.hstack([labels, torch.tensor([[model.config.eos_token_id]])])"
      ],
      "id": "KECoWaGUocoh"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GhQrHNFnerN",
        "outputId": "a53ca338-a1bc-42ba-ea0a-6b5e13bb1683"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 113])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "labels.shape"
      ],
      "id": "7GhQrHNFnerN"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F60-sVtcpH_G",
        "outputId": "d7cfdf53-39f9-4525-c0e2-054a10152428"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 113])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "sample_encoding[\"input_ids\"].shape"
      ],
      "id": "F60-sVtcpH_G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzNMHN7HivSx"
      },
      "source": [
        "#### Now, we wrap the whole encoding into a method"
      ],
      "id": "ZzNMHN7HivSx"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_k_coSUwhr0O"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "import torch\n",
        "import itertools\n",
        "\n",
        "def construct_causalLM_sample(sample: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n",
        "    extended_batch = {}\n",
        "\n",
        "    attended_input_length = sample[\"input_ids\"].shape[-1]\n",
        "\n",
        "    extended_batch[\"input_ids\"] = sample[\"input_ids\"].expand(attended_input_length, -1)\n",
        "\n",
        "    extended_batch[\"attention_mask\"] = torch.tril(torch.ones(attended_input_length, attended_input_length), diagonal=0)\n",
        "\n",
        "    extended_batch[\"labels\"] = sample[\"input_ids\"][..., 1:][:attended_input_length]\n",
        "    extended_batch[\"labels\"] = torch.hstack([extended_batch[\"labels\"][0], torch.tensor([model.config.eos_token_id])])\n",
        "\n",
        "    extended_batch[\"labels_position\"] = torch.arange(sample[\"input_ids\"].shape[-1])\n",
        "\n",
        "    return extended_batch"
      ],
      "id": "_k_coSUwhr0O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTYdZ21vJybl"
      },
      "source": [
        "### 3: Iterate over the samples and 4: Update the model\n",
        "\n",
        "Finally, we plug in the processing into the large training loop.\n",
        "\n",
        "#### What is happening here?\n",
        "\n",
        "As in training any neural network, we need to take care of several things that are not directly related to our objective.\n",
        "\n",
        "* Configure **batch size** and **learning rate**\n",
        "* Initialize **optimizer** that updates the model according to the gradients of the loss from real data\n",
        "* Initialize **loss function** measuring how well the model fits the data\n",
        "\n",
        "After that we **iterate over data**:\n",
        "* Obtain **batches of CLM samples**\n",
        "* Run them through the model to **obtain predictions** in a form of (log) probabilities over the model's vocabulary\n",
        "* Compute the value of the loss and register gradients of the model weights used later to update the model\n",
        "* Update the model and restart the gradients\n",
        "* Finally, we stop if the training does not improve for a while"
      ],
      "id": "kTYdZ21vJybl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a66c18f-8e39-4acf-af33-37ca69c46582",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-6\n",
        "\n",
        "optimizer = AdamW(model.parameters(),  # optimizer will actually update the model weights,\n",
        "                  no_deprecation_warning=True,  # so that they get better at prediction after every step\n",
        "                  lr=learning_rate)\n",
        "\n",
        "loss_fn = CrossEntropyLoss()  # distance function comparing predictions to expected labels\n",
        "\n",
        "while True:\n",
        "    running_loss = 0  # aggregation variable, to observe if we progress\n",
        "    last_running_loss = 10e28  # super high initial loss that will decrease\n",
        "    for text in dataset[\"unsupervised\"]['text']:  # per-sample iteration\n",
        "        sample_encoding = tokenizer(text, \n",
        "                                    padding=\"longest\",  # padding and truncation allows us to directly obtain tensors,\n",
        "                                    truncation=True,    # but otherwise are not needed\n",
        "                                    return_tensors=\"pt\")\n",
        "        \n",
        "        clm_samples = construct_causalLM_sample(sample_encoding)  # transformation to CLM samples\n",
        "        for batch_offset in range(0, len(clm_samples[\"input_ids\"]), batch_size):  # per-CLM-samples iteration, batched\n",
        "\n",
        "            # Construction of the training batch that we've seen above\n",
        "            dataset_batch = {k: clm_samples[k][batch_offset: batch_offset+batch_size].to(model.device) \n",
        "                             for k in clm_samples.keys()}\n",
        "\n",
        "            # Model prediction, (also called forward pass)\n",
        "            model_logprobs = model(input_ids=dataset_batch[\"input_ids\"],  # this can also be done with model(**dataset_batch)\n",
        "                                   attention_mask=dataset_batch[\"attention_mask\"]).logits          \n",
        "            # HuggingFace implementation gives us predictions for all tokens, \n",
        "            # but we'll update the model only based on the predictions with labels \n",
        "            logprobs_with_labels = model_logprobs[torch.arange(model_logprobs.size(0)), dataset_batch[\"labels_position\"]]\n",
        "                        \n",
        "            # we first compare the predicted probabilities with \"true probabilities\"\n",
        "            loss_value = loss_fn(logprobs_with_labels, dataset_batch[\"labels\"])\n",
        "\n",
        "            # we note the errors (gradients) to each model parameters (also called backward pass)\n",
        "            loss_value.backward()\n",
        "            \n",
        "            running_loss += loss_value.item()\n",
        "            \n",
        "            # 4. We update the model\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # 5: Evaluation: Check and stop the training if the model no longer improves\n",
        "            if batch_offset!= 0 and (batch_offset/batch_size) % 10 == 0:\n",
        "                # print our loss after every 1000-th step\n",
        "                print(\"Current training loss: %s\" % running_loss)\n",
        "                # stop if the loss increased\n",
        "                if last_running_loss < running_loss:\n",
        "                    break\n",
        "\n",
        "                running_loss = 0  # restart the log"
      ],
      "id": "8a66c18f-8e39-4acf-af33-37ca69c46582"
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the **training log of real large-scale training** of BLOOM (176B params, $2-5M in cloud computing): https://huggingface.co/bigscience/bloom/tensorboard"
      ],
      "metadata": {
        "id": "yZmE55MWviwh"
      },
      "id": "yZmE55MWviwh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45c9f88-bb44-4e74-9828-e6acebbfc8ba"
      },
      "source": [
        "# ðŸ›¥ High-level Training pipeline\n",
        "\n",
        "Today, many libraries make it much easier to train your language model, with different levels of specialized knowledge. Under the hood, it always comes down to (roughly) what we see above, but the high-level interface allows you to iterate experiments much faster.\n",
        "\n",
        "For PyTorch language models, you may consider at least these libraries (incrementally by usage complexity): **Pure PyTorch, PyTorch Lightning, Fairseq, Transformers Trainer, Adaptor (ours)**. We have seen the low-level side above, and now we'll peek into the most high-level (Adaptor). \n",
        "\n",
        "However, in your time, we also strongly recommend you to also take a look at how to use the ever-growing ðŸ¤— HuggingFace Transformers library. You can find examples for training generative models in [Translation training tutorial](https://huggingface.co/docs/transformers/tasks/translation), [Summarization training tutorial](https://huggingface.co/docs/transformers/tasks/summarization) (it's almost the same) and [Generation example script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py) from ðŸ¤— HuggingFace."
      ],
      "id": "f45c9f88-bb44-4e74-9828-e6acebbfc8ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd0b70fa-5f1a-498e-a02b-81b72c83cc1f"
      },
      "source": [
        "### [Adaptor](https://github.com/gaussalgo/adaptor): Quick introduction\n",
        "\n",
        "[Adaptor](https://github.com/gaussalgo/adaptor) is our in-house library that allows us to run large collections of similar experiments very quickly. If you take a look at the [example script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py) above, you'll see that it has over 400 lines, with most code not directly relevant for the goal. Below, you'll see complete, similar example with Adaptor.\n",
        "\n",
        "This complexity reduction is enabled by **objective-centric paradigm**, where the model is no longer the central part of the training; the central structure in Adaptor is **training objective** that is applied to the model.\n",
        "\n",
        "Design-wise, Adaptor is relatively lightweight extension of ðŸ¤— Transformers. Thanks to that, you can use almost all cutting-edge features as well as all the language models of ðŸ¤— Transformers.\n"
      ],
      "id": "bd0b70fa-5f1a-498e-a02b-81b72c83cc1f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOzKykT3RmzT"
      },
      "source": [
        "## Training generative models with Adaptor\n",
        "\n",
        "We will take a look at how the training of generative model will look like if we use Adaptor. \n",
        "\n",
        "To give you an example for the final hands-on, we will demonstrate how to use the library on a fairly simple use-case, where we'll train a LM to **generate a rating of the review** that it gets in the input text. Again, we'll use the same `imdb` dataset for that."
      ],
      "id": "zOzKykT3RmzT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6b1a0f2-cc70-46dd-82bf-72277f44bff4"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets sentencepiece protobuf==3.20.0 adaptor==0.2.2  # required for generation\n",
        "!pip uninstall -y -q tensorflow tensorboard"
      ],
      "id": "b6b1a0f2-cc70-46dd-82bf-72277f44bff4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K98Yfkmj1_W"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"imdb\", split=\"train\")"
      ],
      "id": "6K98Yfkmj1_W"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyChO-yiXgaP"
      },
      "source": [
        "First, we pick the base model for adaptation. "
      ],
      "id": "gyChO-yiXgaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4df1218-66cf-4f27-b156-b6c84f49f11d"
      },
      "outputs": [],
      "source": [
        "from adaptor.lang_module import LangModule\n",
        "\n",
        "language_module = LangModule(\"google/mt5-small\")"
      ],
      "id": "b4df1218-66cf-4f27-b156-b6c84f49f11d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92bcac29-dad5-41da-8967-1fba93de87d3"
      },
      "source": [
        "Second, we choose the objective that we want to fine-tune the model for. The objective will take care of configuring the model correctly. We just give it our desired inputs and outputs."
      ],
      "id": "92bcac29-dad5-41da-8967-1fba93de87d3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ec9dbb0-28ce-4eef-adb4-e7f94fface07"
      },
      "outputs": [],
      "source": [
        "from adaptor.objectives.seq2seq import Sequence2Sequence\n",
        "\n",
        "promt = \"Is this review positive, or negative?\"\n",
        "\n",
        "eval_samples = 100\n",
        "\n",
        "training_objective = Sequence2Sequence(lang_module=language_module,\n",
        "                                       texts_or_path=[promt + review for review in dataset['text']][:-eval_samples],\n",
        "                                       labels_or_path=[\"positive\" if y == 1 else \"negative\" for y in dataset[\"label\"]][:-eval_samples],\n",
        "                                       val_texts_or_path=[promt + review for review in dataset['text']][-eval_samples:],\n",
        "                                       val_labels_or_path=[\"positive\" if y == 1 else \"negative\" for y in dataset[\"label\"]][-eval_samples:],\n",
        "                                       batch_size=1)"
      ],
      "id": "8ec9dbb0-28ce-4eef-adb4-e7f94fface07"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox2abCToZwRn"
      },
      "source": [
        "One more thing before the training: we need to **persist the weights** of the trained model somewhere. In our case, we create checkpoints that can be directly loaded as any HuggingFace model.\n",
        "\n",
        "In Google Colab, you can mount your Google Drive to persist the model checkpoints using the following commands. If you run this script elsewhere, you may skip the following steps."
      ],
      "id": "Ox2abCToZwRn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqKMyUe6xLhb"
      },
      "outputs": [],
      "source": [
        "# This will mount your google drive to persist the training model later on. \n",
        "# If you do not want to do it, you can skip this command.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "id": "JqKMyUe6xLhb"
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content/drive/MyDrive/training_output_dir\"  # TODO: this is a path to your Google Drive - make sure that it is ok to write"
      ],
      "metadata": {
        "id": "4-A1r3lGheQK"
      },
      "id": "4-A1r3lGheQK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fa4070a-64fd-4f4d-a0e0-a095130abc94"
      },
      "outputs": [],
      "source": [
        "# Before starting the training, check that the folder where the model will be persisted actually exist\n",
        "\n",
        "!ls $output_dir"
      ],
      "id": "3fa4070a-64fd-4f4d-a0e0-a095130abc94"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v-Fd9f-3v8B"
      },
      "outputs": [],
      "source": [
        "# if it does not, create it manually in the menu on the right\n",
        "\n",
        "!mkdir -p $output_dir"
      ],
      "id": "_v-Fd9f-3v8B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f3f1c47-66d2-4548-9555-715351459eb5"
      },
      "outputs": [],
      "source": [
        "# Check that the folder for checkpoints exist; Continue, if this command passes without errors\n",
        "\n",
        "!ls $output_dir"
      ],
      "id": "7f3f1c47-66d2-4548-9555-715351459eb5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12cebf2c-6c81-46ce-84f8-e5eb2d8f0380"
      },
      "source": [
        "The training process is configured through a possibly large set of ðŸ¤— Training Arguments. You can read through each of them in [TrainingArgs documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments).\n",
        "\n",
        "We will use our chosen `output_dir` here."
      ],
      "id": "12cebf2c-6c81-46ce-84f8-e5eb2d8f0380"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32bae738-67fb-465d-81c6-a9efae3e138d"
      },
      "outputs": [],
      "source": [
        "from adaptor.utils import AdaptationArguments, StoppingStrategy\n",
        "\n",
        "args = AdaptationArguments(output_dir=output_dir,\n",
        "                           learning_rate=2e-5,\n",
        "                           warmup_steps=100,\n",
        "                           stopping_strategy=StoppingStrategy.FIRST_OBJECTIVE_CONVERGED,\n",
        "                           do_train=True,\n",
        "                           do_eval=True,\n",
        "                           logging_steps=100,\n",
        "                           eval_steps=200,\n",
        "                           evaluation_strategy=\"steps\",\n",
        "                           save_steps=200,\n",
        "                           save_total_limit=2,\n",
        "                           stopping_patience=5,\n",
        "                           num_train_epochs=20,\n",
        "                           max_steps=1000,  # remove this to remove a constraint on a training length\n",
        "                           gradient_accumulation_steps=30)"
      ],
      "id": "32bae738-67fb-465d-81c6-a9efae3e138d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87c21df6-44e3-44f1-9f00-3e6ba9fe2868"
      },
      "source": [
        "The ordering of application of our defined objectives is determined by choosing a `Schedule`: Adaptor comes with `SequentialSchedule` and `ParallelSchedule`.\n",
        "In a single-objective cases (like ours), a selection of Schedule does not really matter, but in multi-task training, it can come quite handy.\n",
        "\n",
        "It is also fine to use more than one objective at once.\n",
        "In such cases, the only extra thing that one needs to decide is if the objectives' heads would be shared or not. If yes, you should fill in the argument `share_other_objective_head=other_training_objective` to the new objective(s)."
      ],
      "id": "87c21df6-44e3-44f1-9f00-3e6ba9fe2868"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2efa3d0-f4bc-4ca4-a9e3-a430cf7c8384"
      },
      "outputs": [],
      "source": [
        "from adaptor.schedules import SequentialSchedule, ParallelSchedule\n",
        "from adaptor.adapter import Adapter\n",
        "\n",
        "# choose a schedule of applying objectives - with one objective does not really matter\n",
        "parallel_schedule = SequentialSchedule(objectives=[training_objective], args=args)\n",
        "\n",
        "# instantiate Adapter - analogical structure to HF Transformers' Trainer\n",
        "adapter = Adapter(lang_module=language_module,\n",
        "                  schedule=parallel_schedule,\n",
        "                  args=args)"
      ],
      "id": "d2efa3d0-f4bc-4ca4-a9e3-a430cf7c8384"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a72e0cd7-464b-470d-82c2-89e142af7d69"
      },
      "source": [
        "After all the configuration, we are ready to run the training and wait for the trained model.\n",
        "\n",
        "Given the `stopping_strategy=StoppingStrategy.FIRST_OBJECTIVE_CONVERGED` and `stopping_patience=1`, the training will terminate after first evaluation, where `model_quality_evaluator` (or evaluation loss, if no Evaluator is given) does not improve over one evaluation."
      ],
      "id": "a72e0cd7-464b-470d-82c2-89e142af7d69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37079d64-8d00-4dcc-a5ed-061655433f01",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "adapter.train()"
      ],
      "id": "37079d64-8d00-4dcc-a5ed-061655433f01"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVVnaWK1pxS-"
      },
      "source": [
        "# âœ‹ Final Hands on: Train your own In-context learner\n",
        "\n",
        "Now your final task will be to improve In-context learning ability for a specific use-case that you have at hand. \n",
        "\n",
        "You can use any of the approaches of the existing models. Additionally, you can use datasets for **related tasks**. If you'd like to create an in-context learner for a **new language**, search if your target language has a QA dataset available. If not, chances are that you can still transfer using QA dataset in a similar language.\n"
      ],
      "id": "aVVnaWK1pxS-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEd93ELE0sJG"
      },
      "source": [
        "### Implementation template\n",
        "\n",
        "Compared to the example of generation above, perhaps all you need to play with are the base model, and inputs and outputs. Think about the *relatedness* of the tasks and relevance of the existing Promptsource templates that you could use.\n",
        "\n",
        "When you are done with the design of your experiment, try executing the plan by optionally filling the template below."
      ],
      "id": "MEd93ELE0sJG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVyFcsWD16K0"
      },
      "outputs": [],
      "source": [
        "base_model = \"google/mt5-small\"  # TODO: pick the base model"
      ],
      "id": "qVyFcsWD16K0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He90YEiS2J4i"
      },
      "outputs": [],
      "source": [
        "from adaptor.lang_module import LangModule\n",
        "\n",
        "language_module = LangModule(base_model)"
      ],
      "id": "He90YEiS2J4i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fTqeyUt2PEF"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "main_dataset = load_dataset(\"squad\")  # TODO: pick datasets: see https://huggingface.co/datasets\n",
        "\n",
        "# other_dataset = load_dataset(\"imdb\")  # maybe do the same thing with your target dataset/similar templates?"
      ],
      "id": "2fTqeyUt2PEF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1LMX15SOHKL"
      },
      "source": [
        "Input prompts & labels collection"
      ],
      "id": "o1LMX15SOHKL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1I5HrXYN5qW"
      },
      "outputs": [],
      "source": [
        "input_texts = []\n",
        "label_texts = []"
      ],
      "id": "J1I5HrXYN5qW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3ZU0AvdPRaw"
      },
      "source": [
        "Using Promptsource to verbalize squad's templates: See all templates on the project repo: https://github.com/bigscience-workshop/promptsource"
      ],
      "id": "B3ZU0AvdPRaw"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/fewshot-goes-multilingual/promptsource.git"
      ],
      "metadata": {
        "id": "83HaBuoMs43U"
      },
      "id": "83HaBuoMs43U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqw2gGuG2_6Q"
      },
      "outputs": [],
      "source": [
        "from promptsource.templates import DatasetTemplates\n",
        "\n",
        "prompts = DatasetTemplates(\"squad\")\n",
        "\n",
        "for template_id in prompts.all_template_names:\n",
        "    promt_template = prompts[template_id]\n",
        "\n",
        "    prompt_label_pairs = main_dataset[\"validation\"].map(lambda row: {\"prompt\": promt_template.apply(row)})[\"prompt\"]\n",
        "\n",
        "    input_texts.extend(prompt for prompt, label in prompt_label_pairs)\n",
        "    label_texts.extend(label for prompt, label in prompt_label_pairs)"
      ],
      "id": "bqw2gGuG2_6Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ4Nw936OwNr"
      },
      "outputs": [],
      "source": [
        "# shuffle the inputs\n",
        "import random\n",
        "\n",
        "data_index = list(range(len(input_texts)))\n",
        "\n",
        "random.shuffle(data_index)\n",
        "\n",
        "input_texts = [input_texts[i] for i in data_index]\n",
        "label_texts = [label_texts[i] for i in data_index]"
      ],
      "id": "YQ4Nw936OwNr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Enlb9bRDOfH6"
      },
      "outputs": [],
      "source": [
        "# dataset objective\n",
        "\n",
        "val_samples = 100\n",
        "\n",
        "seq2seq_squad = Sequence2Sequence(lang_module=language_module,\n",
        "                                  texts_or_path=input_texts[:-100],\n",
        "                                  labels_or_path=label_texts[:-100],\n",
        "                                  val_texts_or_path=input_texts[-100:],\n",
        "                                  val_labels_or_path=label_texts[-100:],\n",
        "                                  batch_size=1)"
      ],
      "id": "Enlb9bRDOfH6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpYYZ56UQA6t"
      },
      "source": [
        "Training as in the example above. [TrainingArguments documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)."
      ],
      "id": "gpYYZ56UQA6t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afnSteLvOfdw"
      },
      "outputs": [],
      "source": [
        "from adaptor.utils import AdaptationArguments, StoppingStrategy\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/training_output_dir\"\n",
        "\n",
        "args = AdaptationArguments(output_dir=output_dir,\n",
        "                           learning_rate=2e-5,\n",
        "                           warmup_steps=1000,\n",
        "                           stopping_strategy=StoppingStrategy.FIRST_OBJECTIVE_CONVERGED,\n",
        "                           do_train=True,\n",
        "                           do_eval=True,\n",
        "                           logging_steps=100,\n",
        "                           eval_steps=200,\n",
        "                           evaluation_strategy=\"steps\",\n",
        "                           save_steps=200,\n",
        "                           save_total_limit=6,\n",
        "                           stopping_patience=5,\n",
        "                           num_train_epochs=20,\n",
        "                           gradient_accumulation_steps=20)"
      ],
      "id": "afnSteLvOfdw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nakdjk9FQbpJ"
      },
      "outputs": [],
      "source": [
        "from adaptor.schedules import ParallelSchedule\n",
        "from adaptor.adapter import Adapter\n",
        "\n",
        "# choose a schedule of applying objectives - with one objective does not really matter\n",
        "parallel_schedule = ParallelSchedule(objectives=[seq2seq_squad], args=args)\n",
        "\n",
        "# instantiate Adapter - analogical structure to HF Transformers' Trainer\n",
        "adapter = Adapter(lang_module=language_module,\n",
        "                  schedule=parallel_schedule,\n",
        "                  args=args)"
      ],
      "id": "Nakdjk9FQbpJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyFbAtCNQcKg"
      },
      "outputs": [],
      "source": [
        "# Start the training!\n",
        "adapter.train()"
      ],
      "id": "xyFbAtCNQcKg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final task - Evaluation\n",
        "\n",
        "After a few minutes of training, evaluate how your new ICL model stand in your task, using your evaluator from previous Hands-on!\n",
        "\n",
        "Copy-paste your implementation of evaluation from **âœ‹ Hands on** in [ICL_intro notebook](https://github.com/gaussalgo/L2L_MLPrague23/blob/main/notebooks/ICL_intro.ipynb) here, change the model path your most recent checkpoint in `output_dir` and run the evaluation again."
      ],
      "metadata": {
        "id": "MpWucMVjmQm0"
      },
      "id": "MpWucMVjmQm0"
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $output_dir"
      ],
      "metadata": {
        "id": "HNUTJLJFqdRb"
      },
      "id": "HNUTJLJFqdRb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        " \n",
        "model_path = output_dir+\"/checkpoint-1000/Sequence2Sequence\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "sdrdExMkx6Sq"
      },
      "id": "sdrdExMkx6Sq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: copy-paste and run your evaluation here"
      ],
      "metadata": {
        "id": "BfyJEVaamjaZ"
      },
      "id": "BfyJEVaamjaZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Take-home exercise\n",
        "\n",
        "Evaluate how your new ICL model stand on the tasks that your model has never seen before. \n",
        "\n",
        "All you need for it is to execute the prepared evaluation script;\n",
        "See **competition** in the [project repository](https://github.com/gaussalgo/L2L_MLPrague23) for details!"
      ],
      "metadata": {
        "id": "dZhLzg0bmjxP"
      },
      "id": "dZhLzg0bmjxP"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}