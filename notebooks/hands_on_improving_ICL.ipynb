{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaussalgo/L2L_MLPrague23/blob/main/notebooks/hands_on_improving_ICL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9f30c1a-8962-4236-b7aa-3718d08548da",
      "metadata": {
        "id": "c9f30c1a-8962-4236-b7aa-3718d08548da"
      },
      "source": [
        "# Training generative models\n",
        "\n",
        "This notebook will show you how to fine-tune existing few-shot in-context learners to perform better on the application of your interest.\n",
        "\n",
        "We will demonstrate how you can fine-tune a model to perform in-context few-shot learning **on a new (target) language** using QA dataset(s) of the target language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "42d0299e-e0b4-4e36-8b1d-2322b68cab54",
      "metadata": {
        "id": "42d0299e-e0b4-4e36-8b1d-2322b68cab54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e1c2359-c8ec-45f1-cf89-e8a701b795de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# notebook's requirements\n",
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32614ce5",
      "metadata": {
        "id": "32614ce5"
      },
      "source": [
        "## Existing models\n",
        "\n",
        "* T5 models (https://huggingface.co/t5-large) - pre-trained on mixture of tasks in text2text format\n",
        "* FLAN - T5 models (https://huggingface.co/google/flan-t5-large) - fine-tuned T5 model on 1000 additional tasks\n",
        "* mT5 models (https://huggingface.co/google/mt5-large) - pre-trained on 101 languages, no supervised tasks - needs to be fine-tuned\n",
        "* Tk models (https://huggingface.co/allenai/tk-instruct-large-def-pos) - fine-tuned T5 on tasks with prompts written as in-context instructions\n",
        "* mTk models (https://huggingface.co/allenai/mtk-instruct-3b-def-pos) - multilingual version of the Tk model\n",
        "* MPT (https://huggingface.co/mosaicml/mpt-7b)\n",
        "* Alpaca (https://github.com/tatsu-lab/stanford_alpaca)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b487bae2-bbf2-44c4-9ea6-70126e319792",
      "metadata": {
        "id": "b487bae2-bbf2-44c4-9ea6-70126e319792",
        "tags": []
      },
      "source": [
        "## Generation - refresher\n",
        "\n",
        "Recall the **Causal Langauge Modeling (CLM)** from earlier, where the model predicts the **following token** from previous context.\n",
        "\n",
        "![image.png](https://gcdnb.pbrd.co/images/Bx4h6Lordx0y.png?o=1)  \n",
        "![image.png](https://gcdnb.pbrd.co/images/rb7bmZS11gtl.png?o=1)\n",
        "![image.png](https://gcdnb.pbrd.co/images/gXYffjzLIk7n.png?o=1)\n",
        "\n",
        "[[images source]](https://www.rohanawhad.com/improvements-of-spanbert-over-bert/)\n",
        "\n",
        "Note that the task of language **generation** is very similar to CLM: We predict the next token conditionally to the input. Only in the case of generation, the **input also contains the previous outputs** of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ab407ee-eedc-4843-9d58-a372ce76b86a",
      "metadata": {
        "id": "6ab407ee-eedc-4843-9d58-a372ce76b86a"
      },
      "source": [
        "## Construction of Training pipeline\n",
        "\n",
        "Many libraries makes it easy to train your NN model, with different levels of user complexity - do not get confused by that.\n",
        "\n",
        "Ordered by implementation complexity incrementally, for PyTorch models, you can find at least these: **Pure PyTorch, PyTorch Lightning, Transformers Trainer, Adaptor (ours)**. We will take a look at how it looks at the most low-level (Pure PyTorch), and the most high-level (Adaptor), but in your time, you can also look at [Sequence Classification tutorial](https://huggingface.co/docs/transformers/tasks/sequence_classification) from HuggingFace.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95be66f8-ce1c-43a5-a49d-91832e69541c",
      "metadata": {
        "id": "95be66f8-ce1c-43a5-a49d-91832e69541c"
      },
      "source": [
        "## â›µ Low-level Training pipeline - Example\n",
        "\n",
        "Here, we are going to take a look at the low level of updating a network.\n",
        "The process can be summarized in the following steps:\n",
        "\n",
        "1. **We pick our base model** to fine-tune. While all Transformer LMs can perform token classification, not all of them are equally good at it. We'll talk about it a bit more offline.\n",
        "\n",
        "2. **We construct training dataset** from our data. Here, we transform texts into valid model inputs/samples (as we've seen in previous session) and assign true labels for each sample.\n",
        "\n",
        "3. **We iterate over the samples** in so-called *epochs*. In this step, we get the model predictions for a *batch* of samples: The raw predictions take form of probabilities (usually log-probabilities, to make prediction faster).\n",
        "\n",
        "4. **We update the model**. Here, we first compare the predicted probabilities with \"true probabilities\", where true category gets a probability==1, and other categories get 0. The comparison is done by so-called *loss* function, which is a special version of distance measure. Then, we update the weights of the model so that they improve the loss metric.\n",
        "\n",
        "5. **We continue as long as the model improves**, which we measure on a held-out dataset, to avoid that the model just learns to remember our data (but then would perform badly in the real world)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1: Pick our base model"
      ],
      "metadata": {
        "id": "aE7HFMhTExJ0"
      },
      "id": "aE7HFMhTExJ0"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fe3494eb-b0c4-4756-b391-4277eb537e03",
      "metadata": {
        "id": "fe3494eb-b0c4-4756-b391-4277eb537e03",
        "outputId": "712d5f82-7462-476b-c4b3-be8a6fb1aad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': 'pad'})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2: Constructing training dataset"
      ],
      "metadata": {
        "id": "FSJXMP6KEq-F"
      },
      "id": "FSJXMP6KEq-F"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "84e44004-d65f-41a3-8dbf-1964e0d3ddd5",
      "metadata": {
        "id": "84e44004-d65f-41a3-8dbf-1964e0d3ddd5",
        "outputId": "1147b7ba-e29f-4e65-b756-8473d067a853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "afc71ac437bf4c89b55c834d1b0c5006",
            "83581493784449fd8bde10a7507cc7b4",
            "30fe6c406e4b4799af0a7803cb8535f8",
            "e1b98278bd98466f8bfa5e686070de2b",
            "8c65296a1b684476b23373930801b0a5",
            "e74acfb357864b68a11dfde993fa6f1d",
            "3ac0f204eb854e92ba706b024f8641b1",
            "6b8ce48c0526474bae7347dd74e37c61",
            "b62d7ba47fa243278def4c35bfba5ac0",
            "0c2fb44543fe4efc82b6a38480720098",
            "1f7e73bba2ad4323997c11b15691f48e"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afc71ac437bf4c89b55c834d1b0c5006"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 2: We construct training dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_encoding = tokenizer(dataset[\"unsupervised\"]['text'][10], \n",
        "                            padding=\"longest\",  # this assures that all input samples have the same length and can be processed in a batch\n",
        "                            truncation=True,  # this removes parts of inputs that can not fit into the model input\n",
        "                            return_tensors=\"pt\"  # this returns the samples as PyTorch tensors, that we do not have to convert ourselves\n",
        "                            )"
      ],
      "metadata": {
        "id": "b-ZgwraRh5SV"
      },
      "id": "b-ZgwraRh5SV",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Constructing training labels"
      ],
      "metadata": {
        "id": "bx33FdQeE5yH"
      },
      "id": "bx33FdQeE5yH"
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT's input ids, decoded:\n",
        "\"|\".join(tokenizer.batch_decode(sample_encoding[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "xteHowKaD7aW",
        "outputId": "67844606-bfd7-4e72-af9a-7cfc4a1fe5b5"
      },
      "id": "xteHowKaD7aW",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This| isn|'t| the| worst| comedy| of| all|-|time|,| but| that| is| about| the| best| thing| that| I| can| say| about| this| pathetic| film|.| I| didn|'t| laugh| once|,| or| even| smile| once| during| this| bomb|.| There| was| usually| something| going| on| on|-|screen|,| so| I| didn|'t| get| TO|O| bored|,| but| most| of| the| jokes| here| were| simply| awful|.| The| final| sequence| is| nothing| more| than| a| long| series| of| people| falling| through| doors| and| stumbling| all| over| the| place|.| Needless| to| say|,| it| was| a| fitting| way| to| end| a| movie| that| was| impossible| for| me| to| like|.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT's training labels: when training to predict the following token, we simply shift the inputs one position to the right\n",
        "labels = sample_encoding[\"input_ids\"][..., 1:]\n",
        "\n",
        "\"|\".join(tokenizer.batch_decode(labels[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "5v2Mb3FuDqBX",
        "outputId": "80bdb2a3-ca32-4c03-8f84-7622fd457550"
      },
      "id": "5v2Mb3FuDqBX",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" isn|'t| the| worst| comedy| of| all|-|time|,| but| that| is| about| the| best| thing| that| I| can| say| about| this| pathetic| film|.| I| didn|'t| laugh| once|,| or| even| smile| once| during| this| bomb|.| There| was| usually| something| going| on| on|-|screen|,| so| I| didn|'t| get| TO|O| bored|,| but| most| of| the| jokes| here| were| simply| awful|.| The| final| sequence| is| nothing| more| than| a| long| series| of| people| falling| through| doors| and| stumbling| all| over| the| place|.| Needless| to| say|,| it| was| a| fitting| way| to| end| a| movie| that| was| impossible| for| me| to| like|.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating next-token prediction inputs from each sample\n",
        "\n",
        "For the next-token prediction, we actually create multiple samples from each text: There are **many** tokens that we can use as **targets**!\n",
        "\n",
        "To make it easier for us, we'll repeatedly use the **same input ids**, and only **attend to the previous tokens**, to be used in prediction. We just need to be careful not to un-mask the actually-predicted token.\n",
        "\n",
        "We can implement this quite easily by constructing a **triangular attention mask** for each input from the batch."
      ],
      "metadata": {
        "id": "aFxPgvIvNl9T"
      },
      "id": "aFxPgvIvNl9T"
    },
    {
      "cell_type": "code",
      "source": [
        "attended_input_length = sample_encoding[\"attention_mask\"].sum(axis=1)\n",
        "attended_input_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANSt04a0Nrv7",
        "outputId": "d5cac435-6662-417f-e693-f1cad088d7b0"
      },
      "id": "ANSt04a0Nrv7",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([113])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input ids**: duplicate inputs by the number of predicted tokens"
      ],
      "metadata": {
        "id": "eL4S3SOEiJhO"
      },
      "id": "eL4S3SOEiJhO"
    },
    {
      "cell_type": "code",
      "source": [
        "# duplicate inputs by the number of predicted tokens\n",
        "input_ids = sample_encoding[\"input_ids\"].expand(attended_input_length, -1)\n",
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bpxWhHQQAUh",
        "outputId": "763656c8-ddae-4876-88f5-7fdd1e99710a"
      },
      "id": "0bpxWhHQQAUh",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1212, 2125,  470,  ...,  284,  588,   13],\n",
              "        [1212, 2125,  470,  ...,  284,  588,   13],\n",
              "        [1212, 2125,  470,  ...,  284,  588,   13],\n",
              "        ...,\n",
              "        [1212, 2125,  470,  ...,  284,  588,   13],\n",
              "        [1212, 2125,  470,  ...,  284,  588,   13],\n",
              "        [1212, 2125,  470,  ...,  284,  588,   13]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbKQ5_5fnNMJ",
        "outputId": "45bfdef0-0af1-452e-f9e3-24ce54d64911"
      },
      "id": "RbKQ5_5fnNMJ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([113, 113])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention mask**: we create triangles that will mask all future tokens from prediction"
      ],
      "metadata": {
        "id": "5bKQm1DyiLSU"
      },
      "id": "5bKQm1DyiLSU"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "\n",
        "# this is how we construct tensor triangles\n",
        "torch.tril(torch.ones(4, 6), diagonal=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEA8oIf7iYMr",
        "outputId": "4d2b9554-7aad-4ad7-eb31-4f6d9b0ea32a"
      },
      "id": "uEA8oIf7iYMr",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_triangle = torch.tril(torch.ones(attended_input_length, input_ids.shape[1]), diagonal=0)\n",
        "attention_triangle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQvzkxXLiLxk",
        "outputId": "f41a438c-bc2f-42ee-d195-b32ba2965cb9"
      },
      "id": "HQvzkxXLiLxk",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_triangle.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFyZ1L2znMHP",
        "outputId": "768bd993-1c35-44b2-a074-f4c4999069d6"
      },
      "id": "rFyZ1L2znMHP",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([113, 113])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Labels**: Finally, we spread the pre-computed labels to assign exactly one label id to each new sample"
      ],
      "metadata": {
        "id": "e6HZwI09iVGD"
      },
      "id": "e6HZwI09iVGD"
    },
    {
      "cell_type": "code",
      "source": [
        "labels = sample_encoding[\"input_ids\"][..., 1:][:attended_input_length]\n",
        "labels\n",
        "\"|\".join(tokenizer.batch_decode(labels[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "VJrQS0Z1iVmi",
        "outputId": "529c09c4-b8e1-4c7b-d910-71195eb4fa9e"
      },
      "id": "VJrQS0Z1iVmi",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" isn|'t| the| worst| comedy| of| all|-|time|,| but| that| is| about| the| best| thing| that| I| can| say| about| this| pathetic| film|.| I| didn|'t| laugh| once|,| or| even| smile| once| during| this| bomb|.| There| was| usually| something| going| on| on|-|screen|,| so| I| didn|'t| get| TO|O| bored|,| but| most| of| the| jokes| here| were| simply| awful|.| The| final| sequence| is| nothing| more| than| a| long| series| of| people| falling| through| doors| and| stumbling| all| over| the| place|.| Needless| to| say|,| it| was| a| fitting| way| to| end| a| movie| that| was| impossible| for| me| to| like|.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After the model is done, we want it to generate a special <EoS> token. This way, we know that the model is done with generation.\n",
        "model.config.eos_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARYScE-vnq9d",
        "outputId": "fcffa389-eb65-4e43-96cd-d207ee71404e"
      },
      "id": "ARYScE-vnq9d",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hence, we add the token as the last label\n",
        "labels = torch.hstack([labels, torch.tensor([[model.config.eos_token_id]])])"
      ],
      "metadata": {
        "id": "KECoWaGUocoh"
      },
      "id": "KECoWaGUocoh",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GhQrHNFnerN",
        "outputId": "1b2e6f32-8a15-46e7-8420-171188c2cc31"
      },
      "id": "7GhQrHNFnerN",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 113])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_encoding[\"input_ids\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F60-sVtcpH_G",
        "outputId": "aeba61a0-6571-4825-c248-1b170287b6c1"
      },
      "id": "F60-sVtcpH_G",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 113])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now, we wrap the whole encoding into a method"
      ],
      "metadata": {
        "id": "ZzNMHN7HivSx"
      },
      "id": "ZzNMHN7HivSx"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "import torch\n",
        "import itertools\n",
        "\n",
        "def construct_causalLM_sample(sample: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n",
        "    extended_batch = {}\n",
        "\n",
        "    attended_input_length = sample[\"input_ids\"].shape[-1]\n",
        "\n",
        "    extended_batch[\"input_ids\"] = sample[\"input_ids\"].expand(attended_input_length, -1)\n",
        "\n",
        "    extended_batch[\"attention_mask\"] = torch.tril(torch.ones(attended_input_length, attended_input_length), diagonal=0)\n",
        "\n",
        "    extended_batch[\"labels\"] = sample[\"input_ids\"][..., 1:][:attended_input_length]\n",
        "    extended_batch[\"labels\"] = torch.hstack([extended_batch[\"labels\"][0], torch.tensor([model.config.eos_token_id])])\n",
        "\n",
        "    extended_batch[\"labels_position\"] = torch.arange(1, sample[\"input_ids\"].shape[-1])\n",
        "\n",
        "    return extended_batch"
      ],
      "metadata": {
        "id": "_k_coSUwhr0O"
      },
      "id": "_k_coSUwhr0O",
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3: Iterate over the samples and 4: Update the model"
      ],
      "metadata": {
        "id": "kTYdZ21vJybl"
      },
      "id": "kTYdZ21vJybl"
    },
    {
      "cell_type": "code",
      "source": [
        "{k: v.shape for k, v in dataset_batch.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARIUnFCmpxNa",
        "outputId": "9e021af2-93b9-47f5-d40a-ca0f8e7081d6"
      },
      "id": "ARIUnFCmpxNa",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': torch.Size([2, 201]),\n",
              " 'attention_mask': torch.Size([2, 201]),\n",
              " 'labels': torch.Size([2])}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_outputs.logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm_27kQ4shwO",
        "outputId": "42cf72b1-4311-4ac8-850c-6cb8068e98c7"
      },
      "id": "tm_27kQ4shwO",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 201, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "8a66c18f-8e39-4acf-af33-37ca69c46582",
      "metadata": {
        "id": "8a66c18f-8e39-4acf-af33-37ca69c46582",
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "ee13fcae-dae3-403c-ef96-9275a4092ac8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-137-b9d1f73e6224>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Model prediction, (also called forward pass)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             model_logprobs = model(input_ids=dataset_batch[\"input_ids\"],  # this can also be done with model(**dataset_batch)\n\u001b[0m\u001b[1;32m     32\u001b[0m                                    attention_mask=dataset_batch[\"attention_mask\"]).logits          \n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# HuggingFace implementation gives us predictions for all tokens,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1096\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "batch_size = 2\n",
        "max_num_epochs = 5\n",
        "learning_rate = 2e-6\n",
        "\n",
        "optimizer = AdamW(model.parameters(),  # optimizer will actually update the model weights,\n",
        "                  no_deprecation_warning=True,  # so that they get better at prediction after every step\n",
        "                  lr=learning_rate)\n",
        "\n",
        "loss_fn = CrossEntropyLoss()  # distance function comparing predictions to expected labels\n",
        "\n",
        "for epoch in range(max_num_epochs):\n",
        "    running_loss = 0\n",
        "    last_running_loss = 10e28\n",
        "    for text in dataset[\"unsupervised\"]['text']:\n",
        "        sample_encoding = tokenizer(text, \n",
        "                                    padding=\"longest\",  # this assures that all input samples have the same length and can be processed in a batch\n",
        "                                    truncation=True,  # this removes parts of inputs that can not fit into the model input\n",
        "                                    return_tensors=\"pt\"  # this returns the samples as PyTorch tensors, that we do not have to convert ourselves\n",
        "                                    )\n",
        "        clm_samples = construct_causalLM_sample(sample_encoding)\n",
        "        for batch_offset in range(0, len(clm_samples[\"input_ids\"]), batch_size):\n",
        "\n",
        "            # the Construction of the training batch that we've seen above\n",
        "            dataset_batch = {k: clm_samples[k][batch_offset: batch_offset+batch_size] \n",
        "                             for k in clm_samples.keys()}\n",
        "\n",
        "            # Model prediction, (also called forward pass)\n",
        "            model_logprobs = model(input_ids=dataset_batch[\"input_ids\"],  # this can also be done with model(**dataset_batch)\n",
        "                                   attention_mask=dataset_batch[\"attention_mask\"]).logits          \n",
        "            # HuggingFace implementation gives us predictions for all tokens, \n",
        "            # but we'll update the model only based on the predictions with labels \n",
        "            logprobs_with_labels = model_logprobs[torch.arange(batch_size), dataset_batch[\"labels_position\"]]\n",
        "                        \n",
        "            # we first compare the predicted probabilities with \"true probabilities\"\n",
        "            loss_value = loss_fn(logprobs_with_labels, dataset_batch[\"labels\"])\n",
        "\n",
        "            # we note the errors (gradients) to each model parameters (also called backward pass)\n",
        "            loss_value.backward()\n",
        "            \n",
        "            running_loss += loss_value.item()\n",
        "            \n",
        "            # 4. We update the model\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # 5: Evaluation: Check and stop the training if the model no longer improves\n",
        "            if batch_offset!= 0 and (batch_offset/batch_size) % 1000 == 0:\n",
        "                # print our loss after every 1000-th step\n",
        "                print(\"Current training loss: %s\" % running_loss)\n",
        "                # stop if the loss increased\n",
        "                if last_running_loss < running_loss:\n",
        "                    break\n",
        "\n",
        "                running_loss = 0  # restart the log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lprobs[torch.arange(batch_size), dataset_batch[\"labels_position\"]].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvXf9xuZ4-54",
        "outputId": "42e2773d-6434-449b-9ae6-79365934bb3b"
      },
      "id": "jvXf9xuZ4-54",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_batch[\"input_ids\"].shape, dataset_batch[\"labels\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8MaEB5Mwku8",
        "outputId": "8e95a835-dc29-480c-a7c4-4973d5190811"
      },
      "id": "F8MaEB5Mwku8",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 201]), torch.Size([2]))"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_batch[\"input_ids\"].size(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX8ohwIF4HhL",
        "outputId": "8dbd06c7-dbe2-4aa2-cd05-6ac60fd8069c"
      },
      "id": "YX8ohwIF4HhL",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "201"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.vstack([model_lprobs[:, i] for i in range(dataset_batch[\"input_ids\"].size(1))]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE8F2p0o03DU",
        "outputId": "703ce189-b3b5-4c97-ba64-bee1e3e50d4c"
      },
      "id": "LE8F2p0o03DU",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([402, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones_like(torch.arange(model_lprobs.shape[1])).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxiuw457zvkC",
        "outputId": "6f6c0f9a-51bc-49d6-dbd3-34d1d1d8f5cf"
      },
      "id": "Sxiuw457zvkC",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([201])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones_like(model_lprobs).dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD-FhEhiz2m5",
        "outputId": "50a8edea-934f-4e00-9ce8-f20909da6a39"
      },
      "id": "cD-FhEhiz2m5",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lprobs.gather(1, torch.ones_like(model_lprobs, dtype=torch.int64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtp5HspFyVnT",
        "outputId": "7b80437c-5540-4fe7-ec1f-ebdded44751b"
      },
      "id": "Mtp5HspFyVnT",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ -36.2221,  -36.1904,  -41.0536,  ...,  -41.9787,  -40.6478,\n",
              "           -38.1296],\n",
              "         [ -36.2221,  -36.1904,  -41.0536,  ...,  -41.9787,  -40.6478,\n",
              "           -38.1296],\n",
              "         [ -36.2221,  -36.1904,  -41.0536,  ...,  -41.9787,  -40.6478,\n",
              "           -38.1296],\n",
              "         ...,\n",
              "         [ -36.2221,  -36.1904,  -41.0536,  ...,  -41.9787,  -40.6478,\n",
              "           -38.1296],\n",
              "         [ -36.2221,  -36.1904,  -41.0536,  ...,  -41.9787,  -40.6478,\n",
              "           -38.1296],\n",
              "         [ -36.2221,  -36.1904,  -41.0536,  ...,  -41.9787,  -40.6478,\n",
              "           -38.1296]],\n",
              "\n",
              "        [[-107.7291, -108.0176, -113.2968,  ..., -116.4645, -115.7444,\n",
              "          -110.8654],\n",
              "         [-107.7291, -108.0176, -113.2968,  ..., -116.4645, -115.7444,\n",
              "          -110.8654],\n",
              "         [-107.7291, -108.0176, -113.2968,  ..., -116.4645, -115.7444,\n",
              "          -110.8654],\n",
              "         ...,\n",
              "         [-107.7291, -108.0176, -113.2968,  ..., -116.4645, -115.7444,\n",
              "          -110.8654],\n",
              "         [-107.7291, -108.0176, -113.2968,  ..., -116.4645, -115.7444,\n",
              "          -110.8654],\n",
              "         [-107.7291, -108.0176, -113.2968,  ..., -116.4645, -115.7444,\n",
              "          -110.8654]]], grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_batch[\"labels\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt6F7GlnsXP6",
        "outputId": "f1fe55ac-46d9-47aa-8f23-4af6530d5c54"
      },
      "id": "Yt6F7GlnsXP6",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([318, 655])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f45c9f88-bb44-4e74-9828-e6acebbfc8ba",
      "metadata": {
        "id": "f45c9f88-bb44-4e74-9828-e6acebbfc8ba"
      },
      "source": [
        "## ðŸ›¥ High-level Training pipeline\n",
        "\n",
        "Many libraries makes it easy to train your NN model, with different levels of user complexity - do not get confused by that.\n",
        "\n",
        "Ordered by implementation complexity incrementally, for PyTorch models, you can find at least these: **Pure PyTorch, PyTorch Lightning, Transformers Trainer, Adaptor (ours)**. We will take a look at how it looks at the most low-level (Pure PyTorch), and the most high-level (Adaptor), but in your time, you can also look at [Generation training tutorial]() and [Example script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py) from HuggingFace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd0b70fa-5f1a-498e-a02b-81b72c83cc1f",
      "metadata": {
        "id": "bd0b70fa-5f1a-498e-a02b-81b72c83cc1f",
        "tags": []
      },
      "source": [
        "## Training Generative model using Adaptor\n",
        "\n",
        "When we train the model for generation, we update it maximise the chance of correctly predicting the following token.\n",
        "\n",
        "During the inference (i.e. the actual usage of the model), we look only at model's prediction at this single token. This is also what `transformers.AnyModelForSequenceClassification` does internally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b1a0f2-cc70-46dd-82bf-72277f44bff4",
      "metadata": {
        "id": "b6b1a0f2-cc70-46dd-82bf-72277f44bff4"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece protobuf==3.20.0 adaptor==0.2.1  # required for generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4df1218-66cf-4f27-b156-b6c84f49f11d",
      "metadata": {
        "id": "b4df1218-66cf-4f27-b156-b6c84f49f11d"
      },
      "outputs": [],
      "source": [
        "from adaptor.lang_module import LangModule\n",
        "\n",
        "language_module = LangModule(\"google/mt5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92bcac29-dad5-41da-8967-1fba93de87d3",
      "metadata": {
        "id": "92bcac29-dad5-41da-8967-1fba93de87d3"
      },
      "source": [
        "Second, we choose the objective that we want to fine-tune the model for. The objective will take care of configuring the model correctly. We just give it our desired inputs and outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec9dbb0-28ce-4eef-adb4-e7f94fface07",
      "metadata": {
        "id": "8ec9dbb0-28ce-4eef-adb4-e7f94fface07"
      },
      "outputs": [],
      "source": [
        "from adaptor.objectives.seq2seq import Sequence2Sequence\n",
        "\n",
        "promt = \"How many stars will this review rate? Options: 1, 2, 3, 4, 5.\"\n",
        "\n",
        "training_objective = Sequence2Sequence(lang_module=language_module,\n",
        "                                       texts_or_path=[promt + review for review in dataset[\"train\"]['comment']],\n",
        "                                       labels_or_path=[str(x) for x in dataset[\"train\"][\"rating_int\"]],\n",
        "                                       val_texts_or_path=[promt + review for review in dataset[\"validation\"]['comment']],\n",
        "                                       val_labels_or_path=[str(x) for x in dataset[\"validation\"][\"rating_int\"]],\n",
        "                                       batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12cebf2c-6c81-46ce-84f8-e5eb2d8f0380"
      },
      "source": [
        "The training process is configured through a possibly large set of arugments. You can read through each of them in [TrainingArgs documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)."
      ],
      "id": "12cebf2c-6c81-46ce-84f8-e5eb2d8f0380"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32bae738-67fb-465d-81c6-a9efae3e138d"
      },
      "outputs": [],
      "source": [
        "from adaptor.utils import AdaptationArguments, StoppingStrategy\n",
        "\n",
        "# TODO: maybe set output dir\n",
        "output_dir = \"/content/drive/MyDrive/training_output_dir\"\n",
        "\n",
        "args = AdaptationArguments(output_dir=output_dir,\n",
        "                           learning_rate=2e-5,\n",
        "                           warmup_steps=1000,\n",
        "                           stopping_strategy=StoppingStrategy.FIRST_OBJECTIVE_CONVERGED,\n",
        "                           do_train=True,\n",
        "                           do_eval=True,\n",
        "                           log_level=\"critical\",\n",
        "                           logging_steps=100,\n",
        "                           eval_steps=200,\n",
        "                           evaluation_strategy=\"steps\",\n",
        "                           save_steps=200,\n",
        "                           save_total_limit=6,\n",
        "                           stopping_patience=5,\n",
        "                           num_train_epochs=20,\n",
        "                           gradient_accumulation_steps=8)"
      ],
      "id": "32bae738-67fb-465d-81c6-a9efae3e138d"
    },
    {
      "cell_type": "markdown",
      "id": "87c21df6-44e3-44f1-9f00-3e6ba9fe2868",
      "metadata": {
        "id": "87c21df6-44e3-44f1-9f00-3e6ba9fe2868"
      },
      "source": [
        "Adaptor allows training for multiple objectives at once, that are applied in a chosen Schedule. Multi-objective training is useful for more complex scenarios, but for our case, we suffice with a single objective, so a selection of Schedule does not really matter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2efa3d0-f4bc-4ca4-a9e3-a430cf7c8384",
      "metadata": {
        "id": "d2efa3d0-f4bc-4ca4-a9e3-a430cf7c8384"
      },
      "outputs": [],
      "source": [
        "from adaptor.schedules import SequentialSchedule\n",
        "from adaptor.adapter import Adapter\n",
        "\n",
        "parallel_schedule = SequentialSchedule(objectives=[training_objective], args=args)\n",
        "# 4. train using Adapter\n",
        "adapter = Adapter(lang_module=language_module,\n",
        "                  schedule=parallel_schedule,\n",
        "                  args=args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last thing before the training: we need to **persist the weights** of the trained model somewhere. In our case, we create checkpoints that can be directly loaded as any HuggingFace model.\n",
        "\n",
        "In Google Colab, you can mount your Google Drive to persist the model checkpoints using the following commands. If you run this script elsewhere, you may skip the following steps."
      ],
      "metadata": {
        "id": "Ox2abCToZwRn"
      },
      "id": "Ox2abCToZwRn"
    },
    {
      "cell_type": "code",
      "source": [
        "# This will mount your google drive to persist the training model later on. \n",
        "# If you do not want to do it, you can skip this command.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JqKMyUe6xLhb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JqKMyUe6xLhb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fa4070a-64fd-4f4d-a0e0-a095130abc94"
      },
      "outputs": [],
      "source": [
        "# TODO: before starting the training, check that the folder where the model will be persisted actually exist\n",
        "\n",
        "!ls $output_dir"
      ],
      "id": "3fa4070a-64fd-4f4d-a0e0-a095130abc94"
    },
    {
      "cell_type": "code",
      "source": [
        "# if it does not, create it manually in the menu on the right\n",
        "\n",
        "# !mkdir $output_dir"
      ],
      "metadata": {
        "id": "_v-Fd9f-3v8B"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_v-Fd9f-3v8B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f3f1c47-66d2-4548-9555-715351459eb5"
      },
      "outputs": [],
      "source": [
        "# Check that the folder for checkpoints existyou can continue, if this command passes without errors\n",
        "\n",
        "!ls $output_dir"
      ],
      "id": "7f3f1c47-66d2-4548-9555-715351459eb5"
    },
    {
      "cell_type": "markdown",
      "id": "a72e0cd7-464b-470d-82c2-89e142af7d69",
      "metadata": {
        "id": "a72e0cd7-464b-470d-82c2-89e142af7d69"
      },
      "source": [
        "After all the configuration, we are ready to run the training and wait for the trained model.\n",
        "\n",
        "Given the `stopping_strategy=StoppingStrategy.FIRST_OBJECTIVE_CONVERGED` and `stopping_patience=1`, the training will terminate after first evaluation, where `model_quality_evaluator` (i.e. `SequenceAccuracy`) does not improve over one evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37079d64-8d00-4dcc-a5ed-061655433f01",
      "metadata": {
        "id": "37079d64-8d00-4dcc-a5ed-061655433f01",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "adapter.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "afc71ac437bf4c89b55c834d1b0c5006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83581493784449fd8bde10a7507cc7b4",
              "IPY_MODEL_30fe6c406e4b4799af0a7803cb8535f8",
              "IPY_MODEL_e1b98278bd98466f8bfa5e686070de2b"
            ],
            "layout": "IPY_MODEL_8c65296a1b684476b23373930801b0a5"
          }
        },
        "83581493784449fd8bde10a7507cc7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e74acfb357864b68a11dfde993fa6f1d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3ac0f204eb854e92ba706b024f8641b1",
            "value": "100%"
          }
        },
        "30fe6c406e4b4799af0a7803cb8535f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b8ce48c0526474bae7347dd74e37c61",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b62d7ba47fa243278def4c35bfba5ac0",
            "value": 3
          }
        },
        "e1b98278bd98466f8bfa5e686070de2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c2fb44543fe4efc82b6a38480720098",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1f7e73bba2ad4323997c11b15691f48e",
            "value": " 3/3 [00:00&lt;00:00,  3.93it/s]"
          }
        },
        "8c65296a1b684476b23373930801b0a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e74acfb357864b68a11dfde993fa6f1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ac0f204eb854e92ba706b024f8641b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b8ce48c0526474bae7347dd74e37c61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b62d7ba47fa243278def4c35bfba5ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c2fb44543fe4efc82b6a38480720098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f7e73bba2ad4323997c11b15691f48e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}