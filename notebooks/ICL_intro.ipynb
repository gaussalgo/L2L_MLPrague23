{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to In-context learning\n",
    "1. **ðŸ¤– What is in-context learning (ICL)**\n",
    "2. **ðŸŽ¨ Prompt design**\n",
    "3. **ðŸ¦® Zero-shot vs. Few-shot ICL**\n",
    "4. **âœ‹ Hands-on: Transforming a dataset into a few-shot prompt-label dataset and evaluating existing models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/fewshot-goes-multilingual/promptsource transformers[sentencepiece]==4.19.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ðŸ¤– In-context learning (ICL)\n",
    "\n",
    "In context learning is a behavior a generative model shows, where it is able to perform never before seen tasks with only its description as a part of the input. \n",
    "\n",
    "This behavior is mainly exhibited by **Large Language models**. The cause of why exactly it occurs is still unknown, but it may have to do with the latent concepts the LM has acquired from pretraining on large amount of data.\n",
    "\n",
    "Learning is not meant as training, instead it means \"understading\" the task solely from the user's input, aka a prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    " \n",
    "model_path = \"gaussalgo/mt5-base-priming-QA_en-cs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The plot is centered around a young Swedish drama student named Lena']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text = \"\"\"\n",
    "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. \n",
    "I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\"\n",
    "I really had to see this for myself. The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life.\n",
    "In particular she wants to focus her attentions to making some sort of documentary on what the\n",
    "average Swede thought about certain political issues such as the Vietnam War and race issues in the United States.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"What is meant by: {}\".format(long_text) # We could be asking about the sentiment of the sentence, or meaning...the instruction is unclear\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "outputs = model.generate(**inputs.to(model.device))\n",
    "outputs_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# predictions:\n",
    "outputs_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸŽ¨ What should the prompts look like?\n",
    "For training a custom in-context learner we need text pairs of a prompt and label. In the above example we see how difficult it can be to create a  prompt. There is art in creating a prompt, that works best with a given model. Below we will present the [promptsource](https://github.com/bigscience-workshop/promptsource) library. Which contains over 2000 prompts for use with 180 different EN datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (/home/nikola/.cache/huggingface/datasets/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-3 Style', 'I wonderâ€¦', 'after_reading', 'based on the following passage', 'based on the previous passage', 'could you tell meâ€¦', 'exam', 'exercise', 'valid_binary', 'yes_no_question']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from promptsource.templates import DatasetTemplates\n",
    "\n",
    "dataset = load_dataset('super_glue', 'boolq', split=\"validation[:10%]\")\n",
    "\n",
    "prompts = DatasetTemplates(\"super_glue/boolq\")\n",
    "print(prompts.all_template_names) # Here you can see all available prompts for the given dataset\n",
    "prompt = prompts['after_reading']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Evaluation\n",
    "Let's evaluate our model on a dataset created using the promptsource library and a dataset about if the answer to a question is in the context. (The model was not trained on this dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "references = [x==1 for x in dataset[\"label\"]]\n",
    "\n",
    "# Get predictions\n",
    "for item in tqdm(dataset):\n",
    "    model_input_string = prompt.apply(item)\n",
    "    inputs = tokenizer(model_input_string,padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs.to(model.device))\n",
    "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    predictions.append(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using 'mt5' classifier; accuracy: 0.23547400611620795\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "correct_predictions = sum([pred == str(true) for pred, true in zip(predictions, references)])\n",
    "incorrect_predictions = sum([pred != str(true) for pred, true in zip(predictions, references)])\n",
    "\n",
    "accuracy = correct_predictions / (correct_predictions+incorrect_predictions)\n",
    "print(\"Prediction using '%s' classifier; accuracy: %s\" % (model.config.model_type, accuracy))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸ¦® Zero-shot vs few-shot in-context learning\n",
    "The prompts we talked about above we all \"zero-shot\" prompts, which means the model had tu learn the task from the prompted instruction and text without any demonstrations on how the expected output should look like. \n",
    "\n",
    "\"Few-shot\" prompting is when we show multiple demonstrations to the model as a part of the input prompt. These input-output examples can considerably up the models performance on never before seen tasks. The demonstration provide a lead on what is the task at hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'positive or negative', 'positive']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_zero_shot = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "input_few_shot_not_heterogenic = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: He said, that the consert was very dull.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: She came from school sad and lonely.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "input_few_shot = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: He said, that the consert was very dull.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: She came from school smiling and singing.\n",
    "Answer:\"positive\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "inputs = tokenizer([input_zero_shot,input_few_shot_not_heterogenic,  input_few_shot], return_tensors=\"pt\", padding=True)\n",
    "outputs = model.generate(**inputs.to(model.device))\n",
    "outputs_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# predictions:\n",
    "outputs_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. âœ‹ Hands on: Creation of an evaluation dataset \n",
    "\n",
    "* Download an existing dataset and transform it into a prompt input - label pair (either by creating your own prompt or by using the promtsource library).\n",
    "  * Text classification (https://huggingface.co/datasets/imdb)\n",
    "  * Named Entity Recognition (https://huggingface.co/datasets/polyglot_ner/viewer/en/train)\n",
    "  * Question Answering (https://huggingface.co/datasets/squad_v2)\n",
    "  * or other\n",
    "\n",
    "* Adjust the evaluation script accordingly\n",
    "\n",
    "* Create a function which will generate a few-shot (the prompt will include few demonstrations of the same task) prompt and label pairs.\n",
    "\n",
    "* evaluate FLAN base model on your dataset (try both zero-shot and few-shot prompts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    " \n",
    "model_path = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_demonstrations():\n",
    "    # From your custom dataset pick random demostrations (prompt-label pairs)\n",
    "    pass\n",
    "\n",
    "def create_few_shot_prompt():\n",
    "    # With the pick_random_demonstrations() function create a new prompt\n",
    "    pass\n",
    "\n",
    "# Get models predictions\n",
    "\n",
    "# Evaluate (depending on your dataset you may need to change the evaluation script) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
