{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to In-context learning\n",
    "1. **ðŸ¤– What is in-context learning (ICL)**\n",
    "2. **ðŸŽ¨ Prompt design**\n",
    "3. **ðŸ¦® Zero-shot vs. Few-shot ICL**\n",
    "4. **âœ‹ Hands-on: Transforming a dataset into a few-shot prompt-label dataset and evaluating existing models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/fewshot-goes-multilingual/promptsource\n",
      "  Cloning https://github.com/fewshot-goes-multilingual/promptsource to /tmp/pip-req-build-9supn9vs\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/fewshot-goes-multilingual/promptsource /tmp/pip-req-build-9supn9vs\n",
      "  Resolved https://github.com/fewshot-goes-multilingual/promptsource to commit eb6d175b2c397cb7ee2aa46c334c17e3238a49cc\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: black<=21.12b0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (21.12b0)\n",
      "Requirement already satisfied: datasets>=1.7.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (2.12.0)\n",
      "Requirement already satisfied: flake8 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (6.0.0)\n",
      "Requirement already satisfied: isort==5.8.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (5.8.0)\n",
      "Requirement already satisfied: jinja2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (3.1.2)\n",
      "Requirement already satisfied: pandas in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (2.0.1)\n",
      "Requirement already satisfied: plotly in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (5.14.1)\n",
      "Requirement already satisfied: py7zr in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (0.20.5)\n",
      "Requirement already satisfied: pytest in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (7.3.1)\n",
      "Requirement already satisfied: pyyaml>=5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (6.0)\n",
      "Requirement already satisfied: requests in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (2.30.0)\n",
      "Requirement already satisfied: streamlit==0.82 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (0.82.0)\n",
      "Requirement already satisfied: altair>=3.2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (5.0.0)\n",
      "Requirement already satisfied: astor in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (0.8.1)\n",
      "Requirement already satisfied: base58 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (2.1.1)\n",
      "Requirement already satisfied: blinker in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (1.6.2)\n",
      "Requirement already satisfied: cachetools>=4.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (5.3.0)\n",
      "Requirement already satisfied: click<8.0,>=7.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (7.1.2)\n",
      "Requirement already satisfied: numpy in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (1.24.3)\n",
      "Requirement already satisfied: packaging in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (9.5.0)\n",
      "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (4.23.1)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (0.8.1b0)\n",
      "Requirement already satisfied: python-dateutil in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (2.8.2)\n",
      "Requirement already satisfied: toml in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (0.10.2)\n",
      "Requirement already satisfied: tornado>=5.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (6.3.2)\n",
      "Requirement already satisfied: tzlocal in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (5.0.1)\n",
      "Requirement already satisfied: validators in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (0.20.0)\n",
      "Requirement already satisfied: gitpython in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (3.1.31)\n",
      "Requirement already satisfied: watchdog in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (3.0.0)\n",
      "Requirement already satisfied: platformdirs>=2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (3.5.1)\n",
      "Requirement already satisfied: tomli<2.0.0,>=0.2.6 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (1.2.3)\n",
      "Requirement already satisfied: pathspec<1,>=0.9.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (0.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (4.5.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (1.0.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.14.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from pandas->promptsource==0.2.3) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from pandas->promptsource==0.2.3) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (2023.5.7)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from flake8->promptsource==0.2.3) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.11.0,>=2.10.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from flake8->promptsource==0.2.3) (2.10.0)\n",
      "Requirement already satisfied: pyflakes<3.1.0,>=3.0.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from flake8->promptsource==0.2.3) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from jinja2->promptsource==0.2.3) (2.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from plotly->promptsource==0.2.3) (8.2.2)\n",
      "Requirement already satisfied: texttable in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (1.6.7)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (3.18.0)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (0.15.7)\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (1.0.0)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (1.0.1)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (0.2.3)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (1.0.9)\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (0.3.1)\n",
      "Requirement already satisfied: psutil in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (5.9.5)\n",
      "Requirement already satisfied: iniconfig in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from pytest->promptsource==0.2.3) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from pytest->promptsource==0.2.3) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from pytest->promptsource==0.2.3) (1.1.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from altair>=3.2.0->streamlit==0.82->promptsource==0.2.3) (4.17.3)\n",
      "Requirement already satisfied: toolz in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from altair>=3.2.0->streamlit==0.82->promptsource==0.2.3) (0.12.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (1.3.1)\n",
      "Requirement already satisfied: filelock in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets>=1.7.0->promptsource==0.2.3) (3.12.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from python-dateutil->streamlit==0.82->promptsource==0.2.3) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from gitpython->streamlit==0.82->promptsource==0.2.3) (4.0.10)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from validators->streamlit==0.82->promptsource==0.2.3) (5.1.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->streamlit==0.82->promptsource==0.2.3) (5.0.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.82->promptsource==0.2.3) (0.19.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/fewshot-goes-multilingual/promptsource"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ðŸ¤– In-context learning (ICL)\n",
    "\n",
    "In context learning is behavior a generative model shows, where it is able to perform never before seen tasks with only its description as a part of the input. \n",
    "\n",
    "This behavior is mainly exhibited by **Large Language models**. The cause of why exactly it occurs is still unknown, but it may have to do with the latent concepts the LM has acquired from pretraining on large amount of data.\n",
    "\n",
    "Learning is not meant as training, instead it means \"understading\" the task solely from the user's input, aka a prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    " \n",
    "model_path = \"gaussalgo/mt5-base-priming-QA_en-cs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The plot is centered around a young Swedish drama student named Lena']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text = \"\"\"\n",
    "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. \n",
    "I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\"\n",
    "I really had to see this for myself. The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life.\n",
    "In particular she wants to focus her attentions to making some sort of documentary on what the\n",
    "average Swede thought about certain political issues such as the Vietnam War and race issues in the United States.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"What is meant by: {}\".format(long_text) # We could be asking about the sentiment of the sentence, or meaning...the instruction is unclear\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "outputs = model.generate(**inputs.to(model.device))\n",
    "outputs_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# predictions:\n",
    "outputs_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸŽ¨ What should the prompts look like?\n",
    "For training a custom in-context learner we need text pairs of a prompt and label. In the above example we see how difficult it can be to create a  prompt. There is art in creating a prompt, that works best with a given model. Below we will present the [promptsource](https://github.com/bigscience-workshop/promptsource) library. Which contains over 2000 prompts for use with 180 different EN datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (/home/nikola/.cache/huggingface/datasets/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-3 Style', 'I wonderâ€¦', 'after_reading', 'based on the following passage', 'based on the previous passage', 'could you tell meâ€¦', 'exam', 'exercise', 'valid_binary', 'yes_no_question']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from promptsource.templates import DatasetTemplates\n",
    "\n",
    "dataset = load_dataset('super_glue', 'boolq', split=\"validation[:10%]\")\n",
    "\n",
    "prompts = DatasetTemplates(\"super_glue/boolq\")\n",
    "print(prompts.all_template_names)\n",
    "prompt = prompts['after_reading']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Evaluation\n",
    "Let's evaluate our model on a dataset created using the promptsource library and a dataset about if the answer to a question is in the context. (The model was not trained on this dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/327 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 327/327 [03:19<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "references = [x==1 for x in dataset[\"label\"]]\n",
    "\n",
    "# Get predictions\n",
    "for item in tqdm(dataset):\n",
    "    model_input_string = prompt.apply(item)\n",
    "    inputs = tokenizer(model_input_string,padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs.to(model.device))\n",
    "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    predictions.append(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using 'mt5' classifier; accuracy: 0.23547400611620795\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "correct_predictions = sum([pred == str(true) for pred, true in zip(predictions, references)])\n",
    "incorrect_predictions = sum([pred != str(true) for pred, true in zip(predictions, references)])\n",
    "\n",
    "accuracy = correct_predictions / (correct_predictions+incorrect_predictions)\n",
    "print(\"Prediction using '%s' classifier; accuracy: %s\" % (model.config.model_type, accuracy))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸ¦® Zero-shot vs few-shot in-context learning\n",
    "The prompts we talked about above we all \"zero-shot\" prompts, which means the model had tu learn the task from the prompted instruction and text without any demonstrations on how the expected output should look like. \n",
    "\n",
    "\"Few-shot\" prompting is when we show multiple demonstrations to the model as a part of the input prompt. These input-output examples can considerably up the models performance on never before seen tasks. The demonstration provide a lead on what is the task at hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'positive or negative', 'positive']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_zero_shot = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "input_few_shot_not_heterogenic = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: He said, that the consert was very dull.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: She came from school sad and lonely.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "input_few_shot = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: He said, that the consert was very dull.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: She came from school smiling and singing.\n",
    "Answer:\"positive\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "inputs = tokenizer([input_zero_shot,input_few_shot_not_heterogenic,  input_few_shot], return_tensors=\"pt\", padding=True)\n",
    "outputs = model.generate(**inputs.to(model.device))\n",
    "outputs_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# predictions:\n",
    "outputs_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. âœ‹ Hands on: Creation of an evaluation dataset \n",
    "\n",
    "* Download an existing dataset and transform it into a prompt input - label pair (either by creating your own prompt or by using the promtsource library).\n",
    "  * Text classification (https://huggingface.co/datasets/imdb)\n",
    "  * Named Entity Recognition (https://huggingface.co/datasets/polyglot_ner/viewer/en/train)\n",
    "  * Question Answering (https://huggingface.co/datasets/squad_v2)\n",
    "  * or other\n",
    "\n",
    "* Adjust the evaluation script accordingly\n",
    "\n",
    "* Create a function which will generate a few-shot (the prompt will include few demonstrations of the same task) prompt and label pairs.\n",
    "\n",
    "* evaluate some existing ICL models on your dataset (try both zero-shot and few-shot prompts):\n",
    "\n",
    "  * https://huggingface.co/google/flan-t5-large\n",
    "  * https://huggingface.co/allenai/mtk-instruct-3b-def-pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_demonstrations():\n",
    "    # From your custom dataset pick random demostrations (prompt-label pairs)\n",
    "    pass\n",
    "\n",
    "def create_few_shot_prompt():\n",
    "    # With the pick_random_demonstrations() function create a new prompt\n",
    "    pass\n",
    "\n",
    "# Get models predictions\n",
    "\n",
    "# Evaluate (depending on your dataset you may need to change the evaluation script) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
