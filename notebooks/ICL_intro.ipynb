{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-context learning (ICL)\n",
    "\n",
    "In context learning is a use of a generative model, where the description of a desired task is a part of the input. \n",
    "\n",
    "While pre-training, the model is trained on a task of \"guessing\" the right word in context. This is achieved by tasks like Masked Language Modeling (MLM) or Causal Language Modeling (CLM). During these tasks, the model aquires an inherent understanding of the language. \n",
    "\n",
    "After pre-training, traditionaly, we would then fine-tune the model through Supervised ML for a specific task for which we need:\n",
    "* Training data (input and label pairs)\n",
    "* Adding a specific layer (\"head\") to the model relevant to our desired task\n",
    "The resulting model is fit for that one specific task.\n",
    "\n",
    "When we talk about ICL, we mean models, which were finetuned using text inputs containing description of desired task(=prompts) and text outputs from several tasks.\n",
    "\n",
    "During inference of these models we provide the prompt. Being trained on a variety of prompts for multiple tasks, the model has a better understanding of the description of the task itself and thus may show ICL capability even on never before seen tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting git+https://github.com/fewshot-goes-multilingual/promptsource\n",
      "  Cloning https://github.com/fewshot-goes-multilingual/promptsource to /tmp/pip-req-build-jaj_w8va\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/fewshot-goes-multilingual/promptsource /tmp/pip-req-build-jaj_w8va\n",
      "  Resolved https://github.com/fewshot-goes-multilingual/promptsource to commit d2dbfda4fc6b1a839908b9daec1431428365fc13\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: black<=21.12b0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (21.12b0)\n",
      "Requirement already satisfied: datasets>=1.7.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (2.12.0)\n",
      "Requirement already satisfied: flake8 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (6.0.0)\n",
      "Requirement already satisfied: isort==5.8.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (5.8.0)\n",
      "Requirement already satisfied: jinja2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (3.1.2)\n",
      "Requirement already satisfied: pandas in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (2.0.1)\n",
      "Requirement already satisfied: plotly in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (5.14.1)\n",
      "Requirement already satisfied: py7zr in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (0.20.5)\n",
      "Requirement already satisfied: pytest in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (7.3.1)\n",
      "Requirement already satisfied: pyyaml>=5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (6.0)\n",
      "Requirement already satisfied: requests in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (2.30.0)\n",
      "Requirement already satisfied: streamlit==0.82 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from promptsource==0.2.3) (0.82.0)\n",
      "Requirement already satisfied: altair>=3.2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (5.0.0)\n",
      "Requirement already satisfied: astor in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (0.8.1)\n",
      "Requirement already satisfied: base58 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (2.1.1)\n",
      "Requirement already satisfied: blinker in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (1.6.2)\n",
      "Requirement already satisfied: cachetools>=4.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (5.3.0)\n",
      "Requirement already satisfied: click<8.0,>=7.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (7.1.2)\n",
      "Requirement already satisfied: numpy in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (1.24.3)\n",
      "Requirement already satisfied: packaging in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (9.5.0)\n",
      "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (4.23.1)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (0.8.1b0)\n",
      "Requirement already satisfied: python-dateutil in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (2.8.2)\n",
      "Requirement already satisfied: toml in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (0.10.2)\n",
      "Requirement already satisfied: tornado>=5.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (6.3.2)\n",
      "Requirement already satisfied: tzlocal in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (5.0.1)\n",
      "Requirement already satisfied: validators in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (0.20.0)\n",
      "Requirement already satisfied: gitpython in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (3.1.31)\n",
      "Requirement already satisfied: watchdog in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (3.0.0)\n",
      "Requirement already satisfied: platformdirs>=2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (3.5.1)\n",
      "Requirement already satisfied: tomli<2.0.0,>=0.2.6 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (1.2.3)\n",
      "Requirement already satisfied: pathspec<1,>=0.9.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (0.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (4.5.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (1.0.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.14.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from pandas->promptsource==0.2.3) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from pandas->promptsource==0.2.3) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (2023.5.7)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from flake8->promptsource==0.2.3) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.11.0,>=2.10.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from flake8->promptsource==0.2.3) (2.10.0)\n",
      "Requirement already satisfied: pyflakes<3.1.0,>=3.0.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from flake8->promptsource==0.2.3) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from jinja2->promptsource==0.2.3) (2.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from plotly->promptsource==0.2.3) (8.2.2)\n",
      "Requirement already satisfied: texttable in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (1.6.7)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (3.18.0)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (0.15.7)\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (1.0.0)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (1.0.1)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (0.2.3)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (1.0.9)\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (0.3.1)\n",
      "Requirement already satisfied: psutil in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (5.9.5)\n",
      "Requirement already satisfied: iniconfig in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from pytest->promptsource==0.2.3) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from pytest->promptsource==0.2.3) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from pytest->promptsource==0.2.3) (1.1.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from altair>=3.2.0->streamlit==0.82->promptsource==0.2.3) (4.17.3)\n",
      "Requirement already satisfied: toolz in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from altair>=3.2.0->streamlit==0.82->promptsource==0.2.3) (0.12.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (1.3.1)\n",
      "Requirement already satisfied: filelock in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets>=1.7.0->promptsource==0.2.3) (3.12.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from python-dateutil->streamlit==0.82->promptsource==0.2.3) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from gitpython->streamlit==0.82->promptsource==0.2.3) (4.0.10)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from validators->streamlit==0.82->promptsource==0.2.3) (5.1.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython->streamlit==0.82->promptsource==0.2.3) (5.0.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==0.82->promptsource==0.2.3) (0.19.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/fewshot-goes-multilingual/promptsource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    " \n",
    "model_path = \"gaussalgo/mt5-base-priming-QA_en-cs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The plot is centered around a young Swedish drama student named Lena']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is meant by: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. \n",
    "I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\"\n",
    "I really had to see this for myself. The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life.\n",
    "In particular she wants to focus her attentions to making some sort of documentary on what the\n",
    "average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. \n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "outputs = model.generate(**inputs.to(model.device))\n",
    "outputs_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# predictions:\n",
    "outputs_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot vs few-shot\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model might understand the task from the input, but it does not know how do we expect it to respond. Therefore, if we have the model adjusted for such use, we can show it the format of the task from a few input-output examples and see if it comprehends.\n",
    "\n",
    "This approach is called in-context few-shot learning: In addition to the description of the task, we give the model a few input-output examples (demonstrations). Given these, the model has much easier time understanding the format of the interaction that we expect from it. The demonstration are the only lead the model has to understand the task at hand. We can see, that if we pick only examples with a negative sentiment, the model is unable to pick the correct label. \n",
    "\n",
    "In this setting, we need to standardize the format of prediction, so that the model can rely on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'positive or negative', 'positive']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_zero_shot = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "input_few_shot_not_heterogenic = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: He said, that the consert was very dull.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: She came from school sad and lonely.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "input_few_shot = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: He said, that the consert was very dull.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: She came from school smiling and singing.\n",
    "Answer:\"positive\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "inputs = tokenizer([input_zero_shot,input_few_shot_not_heterogenic,  input_few_shot], return_tensors=\"pt\", padding=True)\n",
    "outputs = model.generate(**inputs.to(model.device))\n",
    "outputs_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# predictions:\n",
    "outputs_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What should the prompts look like?\n",
    "For training a custom in-context learner we need text pairs of a prompt and label. While in the above example we see a unified prompt, in training it is beneficial to create multiple prompts for one task, as we want to support the models capability to understand the task by its description, not by identifying a task by a template. This diversivication should yield a benefit of having the model understanding never before seen tasks better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (/home/nikola/.cache/huggingface/datasets/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-3 Style', 'I wonder…', 'after_reading', 'based on the following passage', 'based on the previous passage', 'could you tell me…', 'exam', 'exercise', 'valid_binary', 'yes_no_question']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from promptsource.templates import DatasetTemplates\n",
    "\n",
    "dataset = load_dataset('super_glue', 'boolq', split=\"validation[:10%]\")\n",
    "\n",
    "prompts = DatasetTemplates(\"super_glue/boolq\")\n",
    "print(prompts.all_template_names)\n",
    "prompt = prompts['after_reading']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Let's evaluate our model on a dataset created using the promptsource library and a dataset about if the answer to a question is in the context. (The model was not trained on this dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 133/327 [01:21<03:13,  1.00it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "references = [x==1 for x in dataset[\"label\"]]\n",
    "\n",
    "# Get predictions\n",
    "for item in tqdm(dataset):\n",
    "    model_input_string = prompt.apply(item)\n",
    "\n",
    "    inputs = tokenizer(model_input_string,padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs.to(model.device))\n",
    "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    predictions.append(response_text)\n",
    "\n",
    "# Accuracy\n",
    "correct_predictions = sum([pred == str(true) for pred, true in zip(predictions, references)])\n",
    "incorrect_predictions = sum([pred != str(true) for pred, true in zip(predictions, references)])\n",
    "\n",
    "accuracy = correct_predictions / (correct_predictions+incorrect_predictions)\n",
    "print(\"Prediction using '%s' classifier; accuracy: %s\" % (model.config.model_type, accuracy))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✋ [Hands on] Creation of an evaluation dataset \n",
    "\n",
    "* Download an existing dataset and transform it into a prompt input - label pair (either by creating your own prompt or by using the promtsource library).\n",
    "  * Text classification (https://huggingface.co/datasets/imdb)\n",
    "  * Named Entity Recognition (https://huggingface.co/datasets/polyglot_ner/viewer/en/train)\n",
    "  * Question Answering (https://huggingface.co/datasets/squad_v2)\n",
    "  * or other\n",
    "\n",
    "* Adjust the evaluation script accordingly\n",
    "\n",
    "* Create a function which will generate a few-shot (the prompt will include few demonstrations of the same task) prompt and label pairs.\n",
    "\n",
    "* evaluate some existing ICL models on your dataset (try both zero-shot and few-shot prompts):\n",
    "\n",
    "  * https://huggingface.co/google/flan-t5-large\n",
    "  * https://huggingface.co/allenai/mtk-instruct-3b-def-pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
