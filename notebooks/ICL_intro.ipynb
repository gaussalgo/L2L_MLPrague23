{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to In-context learning\n",
    "1. **ðŸ¤– What is in-context learning (ICL)**\n",
    "2. **ðŸ¦® Zero-shot vs. Few-shot ICL**\n",
    "3. **ðŸŽ¨ Prompt design**\n",
    "4. **âœ‹ Hands-on: Transforming a dataset into a few-shot prompt-label dataset and evaluating existing models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/fewshot-goes-multilingual/promptsource"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ðŸ¤– In-context learning (ICL)\n",
    "\n",
    "In context learning is a use of a generative model, where the description of a desired task is a part of the input. \n",
    "\n",
    "While pre-training, the model is trained on a task of \"guessing\" the right word in context. This is achieved by tasks like Masked Language Modeling (MLM) or Causal Language Modeling (CLM). During these tasks, the model aquires an inherent understanding of the language. \n",
    "\n",
    "After pre-training, traditionaly, we would then fine-tune the model through Supervised ML for a specific task for which we need:\n",
    "* Training data (input and label pairs)\n",
    "* Adding a specific layer (\"head\") to the model relevant to our desired task\n",
    "The resulting model is fit for that one specific task.\n",
    "\n",
    "When we talk about ICL, we mean models, which were finetuned using text inputs containing description of desired task(=prompts) and text outputs from several tasks.\n",
    "\n",
    "During inference of these models we provide the prompt. Being trained on a variety of prompts for multiple tasks, the model has a better understanding of the description of the task itself and thus may show ICL capability even on never before seen tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    " \n",
    "model_path = \"gaussalgo/mt5-base-priming-QA_en-cs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikola/L2L_MLPrague23/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The plot is centered around a young Swedish drama student named Lena']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is meant by: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. \n",
    "I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\"\n",
    "I really had to see this for myself. The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life.\n",
    "In particular she wants to focus her attentions to making some sort of documentary on what the\n",
    "average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. \n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "outputs = model.generate(**inputs.to(model.device))\n",
    "outputs_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# predictions:\n",
    "outputs_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ðŸ¦® Zero-shot vs few-shot in-context learning\n",
    "The model might understand the task from the input, but it does not know how do we expect it to respond. Therefore, if we have the model adjusted for such use, we can show it the format of the task from a few input-output examples and see if it comprehends.\n",
    "\n",
    "This approach is called in-context few-shot learning: In addition to the description of the task, we give the model a few input-output examples (demonstrations). Given these, the model has much easier time understanding the format of the interaction that we expect from it. The demonstration are the only lead the model has to understand the task at hand. We can see, that if we pick only examples with a negative sentiment, the model is unable to pick the correct label. \n",
    "\n",
    "In this setting, we need to standardize the format of prediction, so that the model can rely on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'positive or negative', 'positive']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_zero_shot = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "input_few_shot_not_heterogenic = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: He said, that the consert was very dull.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: She came from school sad and lonely.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "input_few_shot = \"\"\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: He said, that the consert was very dull.\n",
    "Answer:\"negative\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: She came from school smiling and singing.\n",
    "Answer:\"positive\"\n",
    "Question: What is the sentiment of the context: positive or negative? \n",
    "Context: I am very happy to be here today.\n",
    "Answer:\"\"\n",
    "\"\"\"\n",
    "inputs = tokenizer([input_zero_shot,input_few_shot_not_heterogenic,  input_few_shot], return_tensors=\"pt\", padding=True)\n",
    "outputs = model.generate(**inputs.to(model.device))\n",
    "outputs_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# predictions:\n",
    "outputs_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ðŸŽ¨ What should the prompts look like?\n",
    "For training a custom in-context learner we need text pairs of a prompt and label. While in the above example we see a unified prompt, in training it is beneficial to create multiple prompts for one task, as we want to support the models capability to understand the task by its description, not by identifying a task by a template. This diversivication should yield a benefit of having the model understanding never before seen tasks better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset super_glue (/home/nikola/.cache/huggingface/datasets/super_glue/boolq/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-3 Style', 'I wonderâ€¦', 'after_reading', 'based on the following passage', 'based on the previous passage', 'could you tell meâ€¦', 'exam', 'exercise', 'valid_binary', 'yes_no_question']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from promptsource.templates import DatasetTemplates\n",
    "\n",
    "dataset = load_dataset('super_glue', 'boolq', split=\"validation[:10%]\")\n",
    "\n",
    "prompts = DatasetTemplates(\"super_glue/boolq\")\n",
    "print(prompts.all_template_names)\n",
    "prompt = prompts['after_reading']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Evaluation\n",
    "Let's evaluate our model on a dataset created using the promptsource library and a dataset about if the answer to a question is in the context. (The model was not trained on this dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 327/327 [03:19<00:00,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using 'mt5' classifier; accuracy: 0.23547400611620795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "references = [x==1 for x in dataset[\"label\"]]\n",
    "\n",
    "# Get predictions\n",
    "for item in tqdm(dataset):\n",
    "    model_input_string = prompt.apply(item)\n",
    "\n",
    "    inputs = tokenizer(model_input_string,padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs.to(model.device))\n",
    "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    predictions.append(response_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using 'mt5' classifier; accuracy: 0.23547400611620795\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "correct_predictions = sum([pred == str(true) for pred, true in zip(predictions, references)])\n",
    "incorrect_predictions = sum([pred != str(true) for pred, true in zip(predictions, references)])\n",
    "\n",
    "accuracy = correct_predictions / (correct_predictions+incorrect_predictions)\n",
    "print(\"Prediction using '%s' classifier; accuracy: %s\" % (model.config.model_type, accuracy))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. âœ‹ Hands on: Creation of an evaluation dataset \n",
    "\n",
    "* Download an existing dataset and transform it into a prompt input - label pair (either by creating your own prompt or by using the promtsource library).\n",
    "  * Text classification (https://huggingface.co/datasets/imdb)\n",
    "  * Named Entity Recognition (https://huggingface.co/datasets/polyglot_ner/viewer/en/train)\n",
    "  * Question Answering (https://huggingface.co/datasets/squad_v2)\n",
    "  * or other\n",
    "\n",
    "* Adjust the evaluation script accordingly\n",
    "\n",
    "* Create a function which will generate a few-shot (the prompt will include few demonstrations of the same task) prompt and label pairs.\n",
    "\n",
    "* evaluate some existing ICL models on your dataset (try both zero-shot and few-shot prompts):\n",
    "\n",
    "  * https://huggingface.co/google/flan-t5-large\n",
    "  * https://huggingface.co/allenai/mtk-instruct-3b-def-pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_demonstrations():\n",
    "    # From your custom dataset pick random demostrations (prompt-label pairs)\n",
    "    pass\n",
    "\n",
    "def create_few_shot_prompt():\n",
    "    # With the pick_random_demonstrations() function create a new prompt\n",
    "    pass\n",
    "\n",
    "# Get models predictions\n",
    "\n",
    "# Evaluate (depending on your dataset you may need to change the evaluation script) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
